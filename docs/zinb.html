<!DOCTYPE html>
<html lang="en">
<head>
<!-- 2018-05-08 Tue 14:21 -->
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1">
<title>Mean/dispersion estimation</title>
<meta name="generator" content="Org mode">
<meta name="author" content="Abhishek Sarkar">
<style type="text/css">
 <!--/*--><![CDATA[/*><!--*/
  .title  { text-align: center;
             margin-bottom: .2em; }
  .subtitle { text-align: center;
              font-size: medium;
              font-weight: bold;
              margin-top:0; }
  .todo   { font-family: monospace; color: red; }
  .done   { font-family: monospace; color: green; }
  .priority { font-family: monospace; color: orange; }
  .tag    { background-color: #eee; font-family: monospace;
            padding: 2px; font-size: 80%; font-weight: normal; }
  .timestamp { color: #bebebe; }
  .timestamp-kwd { color: #5f9ea0; }
  .org-right  { margin-left: auto; margin-right: 0px;  text-align: right; }
  .org-left   { margin-left: 0px;  margin-right: auto; text-align: left; }
  .org-center { margin-left: auto; margin-right: auto; text-align: center; }
  .underline { text-decoration: underline; }
  #postamble p, #preamble p { font-size: 90%; margin: .2em; }
  p.verse { margin-left: 3%; }
  pre {
    border: 1px solid #ccc;
    box-shadow: 3px 3px 3px #eee;
    padding: 8pt;
    font-family: monospace;
    overflow: auto;
    margin: 1.2em;
  }
  pre.src {
    position: relative;
    overflow: visible;
    padding-top: 1.2em;
  }
  pre.src:before {
    display: none;
    position: absolute;
    background-color: white;
    top: -10px;
    right: 10px;
    padding: 3px;
    border: 1px solid black;
  }
  pre.src:hover:before { display: inline;}
  /* Languages per Org manual */
  pre.src-asymptote:before { content: 'Asymptote'; }
  pre.src-awk:before { content: 'Awk'; }
  pre.src-C:before { content: 'C'; }
  /* pre.src-C++ doesn't work in CSS */
  pre.src-clojure:before { content: 'Clojure'; }
  pre.src-css:before { content: 'CSS'; }
  pre.src-D:before { content: 'D'; }
  pre.src-ditaa:before { content: 'ditaa'; }
  pre.src-dot:before { content: 'Graphviz'; }
  pre.src-calc:before { content: 'Emacs Calc'; }
  pre.src-emacs-lisp:before { content: 'Emacs Lisp'; }
  pre.src-fortran:before { content: 'Fortran'; }
  pre.src-gnuplot:before { content: 'gnuplot'; }
  pre.src-haskell:before { content: 'Haskell'; }
  pre.src-hledger:before { content: 'hledger'; }
  pre.src-java:before { content: 'Java'; }
  pre.src-js:before { content: 'Javascript'; }
  pre.src-latex:before { content: 'LaTeX'; }
  pre.src-ledger:before { content: 'Ledger'; }
  pre.src-lisp:before { content: 'Lisp'; }
  pre.src-lilypond:before { content: 'Lilypond'; }
  pre.src-lua:before { content: 'Lua'; }
  pre.src-matlab:before { content: 'MATLAB'; }
  pre.src-mscgen:before { content: 'Mscgen'; }
  pre.src-ocaml:before { content: 'Objective Caml'; }
  pre.src-octave:before { content: 'Octave'; }
  pre.src-org:before { content: 'Org mode'; }
  pre.src-oz:before { content: 'OZ'; }
  pre.src-plantuml:before { content: 'Plantuml'; }
  pre.src-processing:before { content: 'Processing.js'; }
  pre.src-python:before { content: 'Python'; }
  pre.src-R:before { content: 'R'; }
  pre.src-ruby:before { content: 'Ruby'; }
  pre.src-sass:before { content: 'Sass'; }
  pre.src-scheme:before { content: 'Scheme'; }
  pre.src-screen:before { content: 'Gnu Screen'; }
  pre.src-sed:before { content: 'Sed'; }
  pre.src-sh:before { content: 'shell'; }
  pre.src-sql:before { content: 'SQL'; }
  pre.src-sqlite:before { content: 'SQLite'; }
  /* additional languages in org.el's org-babel-load-languages alist */
  pre.src-forth:before { content: 'Forth'; }
  pre.src-io:before { content: 'IO'; }
  pre.src-J:before { content: 'J'; }
  pre.src-makefile:before { content: 'Makefile'; }
  pre.src-maxima:before { content: 'Maxima'; }
  pre.src-perl:before { content: 'Perl'; }
  pre.src-picolisp:before { content: 'Pico Lisp'; }
  pre.src-scala:before { content: 'Scala'; }
  pre.src-shell:before { content: 'Shell Script'; }
  pre.src-ebnf2ps:before { content: 'ebfn2ps'; }
  /* additional language identifiers per "defun org-babel-execute"
       in ob-*.el */
  pre.src-cpp:before  { content: 'C++'; }
  pre.src-abc:before  { content: 'ABC'; }
  pre.src-coq:before  { content: 'Coq'; }
  pre.src-groovy:before  { content: 'Groovy'; }
  /* additional language identifiers from org-babel-shell-names in
     ob-shell.el: ob-shell is the only babel language using a lambda to put
     the execution function name together. */
  pre.src-bash:before  { content: 'bash'; }
  pre.src-csh:before  { content: 'csh'; }
  pre.src-ash:before  { content: 'ash'; }
  pre.src-dash:before  { content: 'dash'; }
  pre.src-ksh:before  { content: 'ksh'; }
  pre.src-mksh:before  { content: 'mksh'; }
  pre.src-posh:before  { content: 'posh'; }
  /* Additional Emacs modes also supported by the LaTeX listings package */
  pre.src-ada:before { content: 'Ada'; }
  pre.src-asm:before { content: 'Assembler'; }
  pre.src-caml:before { content: 'Caml'; }
  pre.src-delphi:before { content: 'Delphi'; }
  pre.src-html:before { content: 'HTML'; }
  pre.src-idl:before { content: 'IDL'; }
  pre.src-mercury:before { content: 'Mercury'; }
  pre.src-metapost:before { content: 'MetaPost'; }
  pre.src-modula-2:before { content: 'Modula-2'; }
  pre.src-pascal:before { content: 'Pascal'; }
  pre.src-ps:before { content: 'PostScript'; }
  pre.src-prolog:before { content: 'Prolog'; }
  pre.src-simula:before { content: 'Simula'; }
  pre.src-tcl:before { content: 'tcl'; }
  pre.src-tex:before { content: 'TeX'; }
  pre.src-plain-tex:before { content: 'Plain TeX'; }
  pre.src-verilog:before { content: 'Verilog'; }
  pre.src-vhdl:before { content: 'VHDL'; }
  pre.src-xml:before { content: 'XML'; }
  pre.src-nxml:before { content: 'XML'; }
  /* add a generic configuration mode; LaTeX export needs an additional
     (add-to-list 'org-latex-listings-langs '(conf " ")) in .emacs */
  pre.src-conf:before { content: 'Configuration File'; }

  table { border-collapse:collapse; }
  caption.t-above { caption-side: top; }
  caption.t-bottom { caption-side: bottom; }
  td, th { vertical-align:top;  }
  th.org-right  { text-align: center;  }
  th.org-left   { text-align: center;   }
  th.org-center { text-align: center; }
  td.org-right  { text-align: right;  }
  td.org-left   { text-align: left;   }
  td.org-center { text-align: center; }
  dt { font-weight: bold; }
  .footpara { display: inline; }
  .footdef  { margin-bottom: 1em; }
  .figure { padding: 1em; }
  .figure p { text-align: center; }
  .inlinetask {
    padding: 10px;
    border: 2px solid gray;
    margin: 10px;
    background: #ffffcc;
  }
  #org-div-home-and-up
   { text-align: right; font-size: 70%; white-space: nowrap; }
  textarea { overflow-x: auto; }
  .linenr { font-size: smaller }
  .code-highlighted { background-color: #ffff00; }
  .org-info-js_info-navigation { border-style: none; }
  #org-info-js_console-label
    { font-size: 10px; font-weight: bold; white-space: nowrap; }
  .org-info-js_search-highlight
    { background-color: #ffff00; color: #000000; font-weight: bold; }
  .org-svg { width: 90%; }
  /*]]>*/-->
</style>
<link href="site_libs/bootstrap-3.3.5/css/cosmo.min.css" rel="stylesheet" />
<link rel="stylesheet" type="text/css" href="https://www.pirilampo.org/styles/readtheorg/css/htmlize.css"/>
<style type="text/css">body {width: 60em; margin:auto} pre.src {overflow:auto}</style>
<script type="text/javascript">
/*
@licstart  The following is the entire license notice for the
JavaScript code in this tag.

Copyright (C) 2012-2017 Free Software Foundation, Inc.

The JavaScript code in this tag is free software: you can
redistribute it and/or modify it under the terms of the GNU
General Public License (GNU GPL) as published by the Free Software
Foundation, either version 3 of the License, or (at your option)
any later version.  The code is distributed WITHOUT ANY WARRANTY;
without even the implied warranty of MERCHANTABILITY or FITNESS
FOR A PARTICULAR PURPOSE.  See the GNU GPL for more details.

As additional permission under GNU GPL version 3 section 7, you
may distribute non-source (e.g., minimized or compacted) forms of
that code without the copy of the GNU GPL normally required by
section 4, provided you include this license notice and a URL
through which recipients can access the Corresponding Source.


@licend  The above is the entire license notice
for the JavaScript code in this tag.
*/
<!--/*--><![CDATA[/*><!--*/
 function CodeHighlightOn(elem, id)
 {
   var target = document.getElementById(id);
   if(null != target) {
     elem.cacheClassElem = elem.className;
     elem.cacheClassTarget = target.className;
     target.className = "code-highlighted";
     elem.className   = "code-highlighted";
   }
 }
 function CodeHighlightOff(elem, id)
 {
   var target = document.getElementById(id);
   if(elem.cacheClassElem)
     elem.className = elem.cacheClassElem;
   if(elem.cacheClassTarget)
     target.className = elem.cacheClassTarget;
 }
/*]]>*///-->
</script>
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        displayAlign: "center",
        displayIndent: "0em",

        "HTML-CSS": { scale: 100,
                        linebreaks: { automatic: "false" },
                        webFont: "TeX"
                       },
        SVG: {scale: 100,
              linebreaks: { automatic: "false" },
              font: "TeX"},
        NativeMML: {scale: 100},
        TeX: { equationNumbers: {autoNumber: "AMS"},
               MultLineWidth: "85%",
               TagSide: "right",
               TagIndent: ".8em"
             }
});
</script>
<script type="text/javascript"
        src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS_HTML"></script>
</head>
<body>
<div id="content">
<h1 class="title">Mean/dispersion estimation</h1>

<div id="outline-container-orgde98ea2" class="outline-2">
<h2 id="orgde98ea2">Introduction</h2>
<div class="outline-text-2" id="text-orgde98ea2">
<p>
We take a modular approach to call QTLs:
</p>

<ol class="org-ol">
<li>Estimate a mean and a dispersion for each individual</li>
<li>Treat the mean/dispersion as continuous phenotypes and perform QTL mapping</li>
</ol>

<p>
Here, we solve (1).
</p>

<ol class="org-ol">
<li><a href="#org6ace9b3">We implement CPU-based ML estimation</a></li>
<li><a href="#org4f496c8">We estimate per-gene indexes of dispersion</a> accounting for the fact that
data came from multiple individuals</li>
</ol>
</div>
</div>

<div id="outline-container-org83e68c4" class="outline-2">
<h2 id="org83e68c4">Model specification</h2>
<div class="outline-text-2" id="text-org83e68c4">
<p>
Let \(r_{ijk}\) denote the number of molecules for individual \(i\), cell
\(j\), gene \(k\). Let \(R_{ij}\) denote a size factor for each cell.
</p>

<p>
\[ r_{ijk} \sim \pi_{ik} \delta_0(\cdot) + (1 - \pi_{ik})\text{Poisson}(\cdot; R_{ij} \mu_{ik} u_{ijk}) \]
</p>

<p>
\[ u_{ijk} \sim \text{Gamma}(\cdot; \phi_{ik}^{-1}, \phi_{ik}^{-1}) \]
</p>

<p>
Here, \(\mu_{ik}\) is proportional to relative expression (<a href="https://arxiv.org/abs/1104.3889">Pachter 2011</a>), and
\(\phi_{ik}\) is the variance of expression noise.
</p>

<p>
Considering just the Poisson component, marginalizing out \(u\) yields the
log likelihood:
</p>

<p>
\[ l(\cdot) = \ln(1 - \pi_{ik}) + r_{ijk} \ln\left(\frac{R_{ij}\mu_{ik}\phi_{ik}}{1 + R_{ij}\mu_{ik}\phi_{ik}}\right) - \phi_{ik}^{-1} \ln(1 + R_{ij}\mu_{ik}\phi_{ik}) + \ln \Gamma(r_{ijk} + \phi_{ik}^{-1}) - \ln \Gamma(r_{ijk} + 1) - \ln \Gamma(\phi^{-1}) \]
</p>

<p>
Then, marginalizing over the mixture yields the log likelihood:
</p>

<p>
\[ \ln p(r_{ijk} \mid \cdot) = \ln(\pi_{ik} + \exp(l(\cdot)))\ \text{if}\ r_{ijk} = 0 \]
</p>

<p>
\[ \ln p(r_{ijk} \mid \cdot) = l(\cdot)\ \text{otherwise} \]
</p>

<p>
We have enough observations per mean/dispersion parameter that simply
minimizing the negative log likelihood should give reasonable estimates.
</p>

<p>
This model is equivalent to a model where we assume that the underlying rate
is a point-Gamma mixture:
</p>

<p>
\[ r_{ijk} \mid \lambda_{ijk} \sim \mathrm{Poisson}(\cdot; R_{ij}\lambda_{ijk}) \]
</p>

<p>
\[ \lambda_{ijk} \sim \pi_{ik} \delta_0(\cdot) + (1 - \pi_{ik})
  \text{Gamma}(\lambda_{ijk}; \phi_{ik}^{-1}, \phi_{ik}^{-1}\mu_{ik}^{-1}) \]
</p>

<p>
The Gamma component of this mixture corresponds to \(\mu_{ik}u_{ijk}\) in the
model above. Considering just the Gamma component, marginalizing out
\(\lambda\) yields the log likelihood:
</p>

<p>
\[ \tilde{l}(\cdot) = \ln(1 - \pi_{ik}) + r_{ijk} \ln\left(\frac{R_{ij}}{R_{ij} + \phi_{ik}^{-1}\mu_{ik}^{-1}} \right) + \phi_{ik}^{-1} \ln\left(\frac{\phi_{ik}^{-1}\mu_{ik}^{-1}}{R_{ij} + \phi_{ik}^{-1}\mu_{ik}^{-1}}\right) + \ln\Gamma(r_{ijk} + \phi_{ik}^{-1}) - \ln\Gamma(r_{ijk} + 1) - \ln\Gamma(\phi_{ik}^{-1}) \]
</p>

<p>
It is clear \(l = \tilde{l}\), and therefore the marginal likelihoods (over
the mixture components) are also equal.
</p>
</div>
</div>

<div id="outline-container-orgfe16c2c" class="outline-2">
<h2 id="orgfe16c2c">Tensorflow implementation</h2>
<div class="outline-text-2" id="text-orgfe16c2c">
<p>
We optimize all of the parameters together, using one-hot encoding to map
parameters to data points. This makes inference more amenable to running on
the GPU.
</p>

<p>
Use <code>tensorflow</code> to automatically differentiate the negative log likelihood and
perform gradient descent.
</p>

<div class="org-src-container">
<pre class="src src-ipython" id="org628d337"><span class="org-keyword">def</span> <span class="org-function-name">nb_llik</span>(x, mean, inv_disp):
  <span class="org-doc">"""Log likelihood of x distributed as NB</span>

<span class="org-doc">  See Hilbe 2012, eq. 8.10</span>

<span class="org-doc">  mean - mean (&gt; 0)</span>
<span class="org-doc">  inv_disp - inverse dispersion (&gt; 0)</span>

<span class="org-doc">  """</span>
  <span class="org-keyword">return</span> (x * tf.log(mean / inv_disp) -
          x * tf.log(1 + mean / inv_disp) -
          inv_disp * tf.log(1 + mean / inv_disp) +
          tf.lgamma(x + inv_disp) -
          tf.lgamma(inv_disp) -
          tf.lgamma(x + 1))

<span class="org-keyword">def</span> <span class="org-function-name">zinb_llik</span>(x, mean, inv_disp, logodds):
  <span class="org-doc">"""Log likelihood of x distributed as ZINB</span>

<span class="org-doc">  See Hilbe 2012, eq. 11.12, 11.13</span>

<span class="org-doc">  mean - mean (&gt; 0)</span>
<span class="org-doc">  inv_disp - inverse dispersion (&gt; 0)</span>
<span class="org-doc">  logodds - dropout log odds</span>

<span class="org-doc">  """</span>
  <span class="org-comment-delimiter"># </span><span class="org-comment">Important identities:</span>
  <span class="org-comment-delimiter"># </span><span class="org-comment">log(x + y) = log(x) + softplus(y - x)</span>
  <span class="org-comment-delimiter"># </span><span class="org-comment">log(sigmoid(x)) = -softplus(-x)</span>
  <span class="org-variable-name">case_zero</span> = -tf.nn.softplus(-logodds) + tf.nn.softplus(nb_llik(x, mean, inv_disp) + tf.nn.softplus(-logodds))
  <span class="org-variable-name">case_non_zero</span> = -tf.nn.softplus(logodds) + nb_llik(x, mean, inv_disp)
  <span class="org-keyword">return</span> tf.where(tf.less(x, 1), case_zero, case_non_zero)

<span class="org-comment-delimiter"># </span><span class="org-comment">https://github.com/junfengwen/AMSGrad/blob/a00e3f4bcb3ba16b2fe67c75dd8643670bded0c9/optimizers.py</span>

<span class="org-keyword">from</span> tensorflow.python.framework <span class="org-keyword">import</span> ops
<span class="org-keyword">from</span> tensorflow.python.ops <span class="org-keyword">import</span> control_flow_ops
<span class="org-keyword">from</span> tensorflow.python.ops <span class="org-keyword">import</span> math_ops
<span class="org-keyword">from</span> tensorflow.python.ops <span class="org-keyword">import</span> gen_math_ops
<span class="org-keyword">from</span> tensorflow.python.ops <span class="org-keyword">import</span> variable_scope
<span class="org-keyword">from</span> tensorflow.python.training <span class="org-keyword">import</span> optimizer

<span class="org-keyword">class</span> <span class="org-type">AMSGrad</span>(optimizer.Optimizer):
  <span class="org-doc">"""The AMSGrad algorithm in the paper</span>

<span class="org-doc">  Reddi, Kale, Kumar, On the Convergence of Adam and Beyond, ICLR 2018</span>

<span class="org-doc">  https://openreview.net/forum?id=ryQu7f-RZ</span>

<span class="org-doc">  """</span>
  <span class="org-keyword">def</span> <span class="org-function-name">__init__</span>(<span class="org-keyword">self</span>, learning_rate=0.001, beta1=0.9, beta2=0.999,
               epsilon=1e-8, use_locking=<span class="org-constant">False</span>, name=<span class="org-string">"AMSGrad"</span>):
    <span class="org-builtin">super</span>(AMSGrad, <span class="org-keyword">self</span>).__init__(use_locking, name)
    <span class="org-keyword">self</span>._lr = learning_rate
    <span class="org-keyword">self</span>._beta1 = beta1
    <span class="org-keyword">self</span>._beta2 = beta2
    <span class="org-keyword">self</span>._epsilon = epsilon

    <span class="org-keyword">self</span>._lr_t = <span class="org-constant">None</span>
    <span class="org-keyword">self</span>._beta1_t = <span class="org-constant">None</span>
    <span class="org-keyword">self</span>._beta2_t = <span class="org-constant">None</span>
    <span class="org-keyword">self</span>._epsilon_t = <span class="org-constant">None</span>

    <span class="org-keyword">self</span>._beta1_power = <span class="org-constant">None</span>
    <span class="org-keyword">self</span>._beta2_power = <span class="org-constant">None</span>

  <span class="org-keyword">def</span> <span class="org-function-name">_create_slots</span>(<span class="org-keyword">self</span>, var_list):

    <span class="org-variable-name">first_var</span> = <span class="org-builtin">min</span>(var_list, key=<span class="org-keyword">lambda</span> x: x.name)

    <span class="org-variable-name">create_new</span> = <span class="org-keyword">self</span>._beta1_power <span class="org-keyword">is</span> <span class="org-constant">None</span>

    <span class="org-keyword">if</span> create_new:
      <span class="org-keyword">with</span> ops.colocate_with(first_var):
        <span class="org-keyword">self</span>._beta1_power = variable_scope.variable(<span class="org-keyword">self</span>._beta1,
                                                    name=<span class="org-string">"beta1_power"</span>,
                                                    trainable=<span class="org-constant">False</span>)
        <span class="org-keyword">self</span>._beta2_power = variable_scope.variable(<span class="org-keyword">self</span>._beta2,
                                                    name=<span class="org-string">"beta2_power"</span>,
                                                    trainable=<span class="org-constant">False</span>)
    <span class="org-comment-delimiter"># </span><span class="org-comment">Create slots for the first and second moments.</span>
    <span class="org-keyword">for</span> v <span class="org-keyword">in</span> var_list:
      <span class="org-comment-delimiter"># </span><span class="org-comment">first moment est</span>
      <span class="org-keyword">self</span>._zeros_slot(v, <span class="org-string">"first_mom"</span>, <span class="org-keyword">self</span>._name)
      <span class="org-comment-delimiter"># </span><span class="org-comment">second moment est</span>
      <span class="org-keyword">self</span>._zeros_slot(v, <span class="org-string">"second_mom"</span>, <span class="org-keyword">self</span>._name)
      <span class="org-keyword">self</span>._zeros_slot(v, <span class="org-string">"second_mom_max"</span>, <span class="org-keyword">self</span>._name)

  <span class="org-keyword">def</span> <span class="org-function-name">_prepare</span>(<span class="org-keyword">self</span>):
    <span class="org-keyword">self</span>._lr_t = ops.convert_to_tensor(<span class="org-keyword">self</span>._lr)
    <span class="org-keyword">self</span>._beta1_t = ops.convert_to_tensor(<span class="org-keyword">self</span>._beta1)
    <span class="org-keyword">self</span>._beta2_t = ops.convert_to_tensor(<span class="org-keyword">self</span>._beta2)
    <span class="org-keyword">self</span>._epsilon_t = ops.convert_to_tensor(<span class="org-keyword">self</span>._epsilon)
    <span class="org-keyword">self</span>._one_minus_beta1 = ops.convert_to_tensor(1. - <span class="org-keyword">self</span>._beta1)
    <span class="org-keyword">self</span>._one_minus_beta2 = ops.convert_to_tensor(1. - <span class="org-keyword">self</span>._beta2)

  <span class="org-keyword">def</span> <span class="org-function-name">_apply_dense</span>(<span class="org-keyword">self</span>, grad, var):
    <span class="org-comment-delimiter"># </span><span class="org-comment">bias-corrected learning rate</span>
    <span class="org-variable-name">lr</span> = <span class="org-keyword">self</span>._lr_t * math_ops.sqrt(1. - <span class="org-keyword">self</span>._beta2_power) / (1. - <span class="org-keyword">self</span>._beta1_power)
    <span class="org-variable-name">first_mom</span> = <span class="org-keyword">self</span>.get_slot(var, <span class="org-string">"first_mom"</span>)
    <span class="org-variable-name">second_mom</span> = <span class="org-keyword">self</span>.get_slot(var, <span class="org-string">"second_mom"</span>)
    <span class="org-variable-name">second_mom_max</span> = <span class="org-keyword">self</span>.get_slot(var, <span class="org-string">"second_mom_max"</span>)
    <span class="org-variable-name">first_update</span> = first_mom.assign(<span class="org-keyword">self</span>._beta1_t * first_mom +
                                    <span class="org-keyword">self</span>._one_minus_beta1 * grad,
                                    use_locking=<span class="org-keyword">self</span>._use_locking)
    <span class="org-variable-name">second_update</span> = second_mom.assign(<span class="org-keyword">self</span>._beta2_t * second_mom +
                                      <span class="org-keyword">self</span>._one_minus_beta2 * math_ops.square(grad),
                                      use_locking=<span class="org-keyword">self</span>._use_locking)
    <span class="org-comment-delimiter"># </span><span class="org-comment">AMSGrad compared to ADAM</span>
    <span class="org-variable-name">second_max_update</span> = second_mom_max.assign(gen_math_ops.maximum(second_mom_max,
                                                                   second_update))
    <span class="org-variable-name">var_update</span> = var.assign_sub(lr * first_update / (math_ops.sqrt(second_max_update) +
                                                     <span class="org-keyword">self</span>._epsilon_t),
                                use_locking=<span class="org-keyword">self</span>._use_locking)
    <span class="org-keyword">return</span> control_flow_ops.group(*[var_update, first_update,
                                    second_update, second_max_update])

  <span class="org-keyword">def</span> <span class="org-function-name">_apply_sparse</span>(<span class="org-keyword">self</span>, grad, var):
    <span class="org-comment-delimiter"># </span><span class="org-comment">just a copy of the dense case, not properly implemented yet</span>
    <span class="org-keyword">return</span> <span class="org-keyword">self</span>._apply_dense(grad, var)

  <span class="org-keyword">def</span> <span class="org-function-name">_finish</span>(<span class="org-keyword">self</span>, update_ops, name_scope):
    <span class="org-comment-delimiter"># </span><span class="org-comment">Update the power accumulators.</span>
    <span class="org-keyword">with</span> ops.control_dependencies(update_ops):
      <span class="org-keyword">with</span> ops.colocate_with(<span class="org-keyword">self</span>._beta1_power):
        <span class="org-variable-name">update_beta1</span> = <span class="org-keyword">self</span>._beta1_power.assign(
          <span class="org-keyword">self</span>._beta1_power * <span class="org-keyword">self</span>._beta1,
          use_locking=<span class="org-keyword">self</span>._use_locking)
        <span class="org-variable-name">update_beta2</span> = <span class="org-keyword">self</span>._beta2_power.assign(
          <span class="org-keyword">self</span>._beta2_power * <span class="org-keyword">self</span>._beta2_t,
          use_locking=<span class="org-keyword">self</span>._use_locking)
    <span class="org-keyword">return</span> control_flow_ops.group(*update_ops + [update_beta1, update_beta2],
                                  name=name_scope)

<span class="org-keyword">def</span> <span class="org-function-name">fit</span>(umi, onehot, size_factor, design, gene_dropout=<span class="org-constant">False</span>, ind_dropout=<span class="org-constant">False</span>, learning_rate=1e-2, max_epochs=1000):
  <span class="org-doc">"""Return estimated log mean and log dispersion. </span>

<span class="org-doc">  If fitting a zero-inflated model, additionally return dropout log odds.</span>

<span class="org-doc">  umi - count matrix (n x p; float32)</span>
<span class="org-doc">  onehot - mapping of individuals to cells (m x n; float32)</span>
<span class="org-doc">  size_factor - size factor vector (n x 1; float32)</span>
<span class="org-doc">  design - confounder matrix (n x q; float32)</span>
<span class="org-doc">  gene_dropout - fit one dropout parameter per gene</span>
<span class="org-doc">  ind_dropout - fit one dropout parameter per individual</span>

<span class="org-doc">  If ind_dropout is True, gene_dropout must be True, otherwise raises</span>
<span class="org-doc">  ArgumentError.</span>

<span class="org-doc">  Returns:</span>

<span class="org-doc">  log_mean - log mean parameter (m x p)</span>
<span class="org-doc">  log_disp - log dispersion parameter (m x p)</span>
<span class="org-doc">  dropout - dropout log odds (1 x p if gene_dropout, n x p if ind_dropout)</span>

<span class="org-doc">  """</span>
  <span class="org-variable-name">n</span>, <span class="org-variable-name">p</span> = umi.shape
  <span class="org-variable-name">_</span>, <span class="org-variable-name">m</span> = onehot.shape
  <span class="org-variable-name">_</span>, <span class="org-variable-name">k</span> = design.shape

  <span class="org-variable-name">params</span> = <span class="org-builtin">locals</span>()
  <span class="org-variable-name">graph</span> = tf.Graph()
  <span class="org-keyword">with</span> graph.as_default(), graph.device(<span class="org-string">'/gpu:*'</span>):
    <span class="org-variable-name">size_factor</span> = tf.Variable(size_factor, trainable=<span class="org-constant">False</span>)
    <span class="org-variable-name">umi</span> = tf.Variable(umi, trainable=<span class="org-constant">False</span>)
    <span class="org-variable-name">onehot</span> = tf.Variable(onehot, trainable=<span class="org-constant">False</span>)
    <span class="org-variable-name">design</span> = tf.Variable(design, trainable=<span class="org-constant">False</span>)

    <span class="org-variable-name">mean</span> = tf.exp(tf.Variable(tf.zeros([m, p])))
    <span class="org-variable-name">inv_disp</span> = tf.exp(tf.Variable(tf.zeros([m, p])))
    <span class="org-variable-name">beta</span> = tf.Variable(tf.zeros([k, p]))

    <span class="org-keyword">if</span> gene_dropout:
      <span class="org-keyword">if</span> ind_dropout:
        <span class="org-variable-name">dropout_params</span> = tf.Variable(tf.zeros([m, p]))
        <span class="org-variable-name">dropout</span> = tf.matmul(onehot, dropout_params)
      <span class="org-keyword">else</span>:
        <span class="org-variable-name">dropout_params</span> = tf.Variable(tf.zeros([1, p]))
        <span class="org-variable-name">dropout</span> = dropout_params
      <span class="org-variable-name">llik</span> = tf.reduce_mean(
        zinb_llik(umi, size_factor * tf.matmul(onehot, mean) * tf.exp(tf.matmul(design, beta)),
                  tf.matmul(onehot, inv_disp), dropout))
    <span class="org-keyword">elif</span> ind_dropout:
      <span class="org-keyword">raise</span> <span class="org-type">ValueError</span>(<span class="org-string">'Cannot specify individual-specific dropout without gene-specific dropout'</span>)
    <span class="org-keyword">else</span>:
      <span class="org-variable-name">llik</span> = tf.reduce_mean(
        nb_llik(umi, size_factor * tf.matmul(onehot, mean) * tf.exp(tf.matmul(design, beta)),
                tf.matmul(onehot, inv_disp)))

    <span class="org-variable-name">train</span> = AMSGrad(learning_rate=learning_rate).minimize(-llik)
    <span class="org-variable-name">opt</span> = [tf.log(mean), -tf.log(inv_disp)]
    <span class="org-keyword">if</span> gene_dropout:
      opt.append(dropout_params)
    <span class="org-variable-name">curr</span> = <span class="org-builtin">float</span>(<span class="org-string">'-inf'</span>)
    <span class="org-keyword">with</span> tf.Session() <span class="org-keyword">as</span> sess:
      sess.run(tf.global_variables_initializer())
      <span class="org-keyword">for</span> i <span class="org-keyword">in</span> <span class="org-builtin">range</span>(max_epochs):
        <span class="org-variable-name">_</span>, <span class="org-variable-name">update</span> = sess.run([train, llik])
        <span class="org-keyword">if</span> <span class="org-keyword">not</span> np.isfinite(update):
          <span class="org-keyword">raise</span> tf.train.NanLossDuringTrainingError
        <span class="org-keyword">if</span> <span class="org-keyword">not</span> i % 500:
          <span class="org-keyword">print</span>(i, update)
      <span class="org-keyword">return</span> sess.run(opt)
</pre>
</div>
</div>

<div id="outline-container-org460a6e9" class="outline-3">
<h3 id="org460a6e9">Fit ZINB2</h3>
<div class="outline-text-3" id="text-org460a6e9">
<p>
Estimate the parameters of the zero-inflated model assuming dropout per
individual and gene.
</p>

<div class="org-src-container">
<pre class="src src-ipython" id="org2bb0d42">&lt;&lt;zinb-imports&gt;&gt;
&lt;&lt;tf-imports&gt;&gt;
&lt;&lt;tf-zinb-impl&gt;&gt;
&lt;&lt;recode-impl&gt;&gt;
&lt;&lt;read-data-qc&gt;&gt;
<span class="org-variable-name">onehot</span> = recode(annotations, <span class="org-string">'chip_id'</span>)
<span class="org-comment-delimiter"># </span><span class="org-comment">These explain most PVE of circular pseudotime (ref. Joyce Hsiao)</span>
<span class="org-variable-name">cell_cycle_genes</span> = [
  <span class="org-string">'ENSG00000094804'</span>, <span class="org-comment-delimiter"># </span><span class="org-comment">CDC6</span>
  <span class="org-string">'ENSG00000170312'</span>, <span class="org-comment-delimiter"># </span><span class="org-comment">CDK1</span>
  <span class="org-string">'ENSG00000175063'</span>, <span class="org-comment-delimiter"># </span><span class="org-comment">UBE2C</span>
  <span class="org-string">'ENSG00000131747'</span>, <span class="org-comment-delimiter"># </span><span class="org-comment">TOP2A</span>
  <span class="org-string">'ENSG00000197061'</span>, <span class="org-comment-delimiter"># </span><span class="org-comment">HIST1H4C</span>
]
<span class="org-variable-name">cell_cycle</span> = (umi.loc[cell_cycle_genes].values / annotations[<span class="org-string">'mol_hs'</span>].values).reshape(-1, 5)
<span class="org-variable-name">chip</span> = recode(annotations, <span class="org-string">'experiment'</span>)
<span class="org-variable-name">design</span> = np.concatenate([chip, cell_cycle], axis=1)
<span class="org-variable-name">design</span> -= design.mean(axis=0)
<span class="org-variable-name">design</span> /= design.std(axis=0)
<span class="org-variable-name">mean</span>, <span class="org-variable-name">dispersion</span>, <span class="org-variable-name">dropout</span> = fit(
  umi=umi.values.T.astype(np.float32),
  onehot=onehot.astype(np.float32),
  design=np.zeros((onehot.shape[0], 1)).astype(np.float32),
  size_factor=annotations[<span class="org-string">'mol_hs'</span>].astype(np.float32).values.reshape(-1, 1),
  gene_dropout=<span class="org-constant">True</span>,
  ind_dropout=<span class="org-constant">True</span>,
  learning_rate=5e-2,
  max_epochs=4000)

<span class="org-variable-name">index</span> = umi.index
<span class="org-variable-name">header</span> = <span class="org-builtin">sorted</span>(<span class="org-builtin">set</span>(annotations[<span class="org-string">'chip_id'</span>]))
pd.DataFrame(mean.T, index=index, columns=header).to_csv(<span class="org-string">'zi2-log-mu.txt.gz'</span>, sep=<span class="org-string">' '</span>, compression=<span class="org-string">'gzip'</span>)
pd.DataFrame(dispersion.T, index=index, columns=header).to_csv(<span class="org-string">'zi2-log-phi.txt.gz'</span>, sep=<span class="org-string">' '</span>, compression=<span class="org-string">'gzip'</span>)
pd.DataFrame(dropout.T, index=index, columns=header).to_csv(<span class="org-string">'zi2-logodds.txt.gz'</span>, sep=<span class="org-string">' '</span>, compression=<span class="org-string">'gzip'</span>)
</pre>
</div>

<div class="org-src-container">
<pre class="src src-sh">sbatch --partition=gpu --gres=gpu:1 --mem=16G --time=60 --job-name=zinb2 --output=zinb2.out
<span class="org-comment-delimiter">#</span><span class="org-comment">!/bin/bash</span>
<span class="org-builtin">source</span> activate scqtl
python /project2/mstephens/aksarkar/projects/singlecell-qtl/code/tf-zinb.py
</pre>
</div>

<pre class="example">
Submitted batch job 45796717

</pre>

<div class="org-src-container">
<pre class="src src-sh">sbatch --partition=broadwl
<span class="org-comment-delimiter">#</span><span class="org-comment">!/bin/bash</span>
cat &gt;.rsync-filter &lt;&lt;EOF
<span class="org-sh-heredoc">+ */</span>
<span class="org-sh-heredoc">+ zi2*.txt.gz</span>
<span class="org-sh-heredoc">- *</span>
<span class="org-sh-heredoc">EOF</span>
rsync -FFau . /project2/mstephens/aksarkar/projects/singlecell-qtl/data/density-estimation/
</pre>
</div>

<pre class="example">
Submitted batch job 45796357

</pre>
</div>
</div>

<div id="outline-container-orgf3af350" class="outline-3">
<h3 id="orgf3af350">Simulation</h3>
<div class="outline-text-3" id="text-orgf3af350">
<div class="org-src-container">
<pre class="src src-ipython">&lt;&lt;zinb-imports&gt;&gt;
&lt;&lt;tf-imports&gt;&gt;
&lt;&lt;sim-impl2&gt;&gt;
&lt;&lt;tf-zinb-impl&gt;&gt;

<span class="org-keyword">def</span> <span class="org-function-name">evaluate</span>(num_samples, num_mols, num_trials=10):
  <span class="org-comment-delimiter"># </span><span class="org-comment">This will be reset inside the simulation to generate counts, but we need to</span>
  <span class="org-comment-delimiter"># </span><span class="org-comment">fix it to get one design matrix for all the simulated genes</span>
  np.random.seed(1000)
  <span class="org-variable-name">design</span> = np.random.normal(size=(num_samples * num_trials, 1))
  <span class="org-comment-delimiter"># </span><span class="org-comment">def simulate(num_samples, size=None, log_mu=None, log_phi=None, logodds=None, seed=None, design=None, fold=None):</span>
  <span class="org-variable-name">args</span> = [(num_samples * num_trials, num_mols, log_mu, log_phi, logodds, <span class="org-constant">None</span>, design, fold)
          <span class="org-keyword">for</span> log_mu <span class="org-keyword">in</span> np.linspace(-12, -6, 7)
          <span class="org-keyword">for</span> log_phi <span class="org-keyword">in</span> np.linspace(-6, 0, 7)
          <span class="org-keyword">for</span> logodds <span class="org-keyword">in</span> np.linspace(-3, 3, 7)
          <span class="org-keyword">for</span> fold <span class="org-keyword">in</span> np.linspace(1, 1.25, 6)]
  <span class="org-variable-name">umi</span> = np.concatenate([simulate(*a)[0][:,:1] <span class="org-keyword">for</span> a <span class="org-keyword">in</span> args], axis=1)
  <span class="org-variable-name">onehot</span> = np.zeros((num_samples * num_trials, num_trials))
  onehot[np.arange(onehot.shape[0]), np.arange(onehot.shape[0]) // num_samples] = 1

  <span class="org-variable-name">log_mu</span>, <span class="org-variable-name">log_phi</span>, <span class="org-variable-name">logodds</span> = fit(
    umi=umi.astype(np.float32),
    onehot=onehot.astype(np.float32),
    design=design.astype(np.float32),
    size_factor=num_mols * np.ones((num_samples * num_trials, 1)).astype(np.float32),
    gene_dropout=<span class="org-constant">True</span>,
    ind_dropout=<span class="org-constant">True</span>,
    learning_rate=5e-2,
    max_epochs=4000)
  <span class="org-variable-name">result</span> = pd.DataFrame(
    [(a[0] // num_trials, <span class="org-builtin">int</span>(a[1]), <span class="org-builtin">int</span>(a[2]), <span class="org-builtin">int</span>(a[3]), <span class="org-builtin">int</span>(a[4]), a[-1], trial)
     <span class="org-keyword">for</span> a <span class="org-keyword">in</span> args
     <span class="org-keyword">for</span> trial <span class="org-keyword">in</span> <span class="org-builtin">range</span>(num_trials)],
    columns=[<span class="org-string">'num_samples'</span>, <span class="org-string">'num_mols'</span>, <span class="org-string">'log_mu'</span>, <span class="org-string">'log_phi'</span>, <span class="org-string">'logodds'</span>, <span class="org-string">'fold'</span>, <span class="org-string">'trial'</span>])
  <span class="org-comment-delimiter"># </span><span class="org-comment">Important: the results need to be transposed before flattening</span>
  <span class="org-variable-name">result</span>[<span class="org-string">'log_mu_hat'</span>] = log_mu.ravel(order=<span class="org-string">'F'</span>)
  <span class="org-variable-name">result</span>[<span class="org-string">'log_phi_hat'</span>] = log_phi.ravel(order=<span class="org-string">'F'</span>)
  <span class="org-variable-name">result</span>[<span class="org-string">'logodds_hat'</span>] = logodds.ravel(order=<span class="org-string">'F'</span>)
  <span class="org-variable-name">result</span>[<span class="org-string">'mean'</span>] = result[<span class="org-string">'num_mols'</span>] * np.exp(result[<span class="org-string">'log_mu_hat'</span>])
  <span class="org-variable-name">result</span>[<span class="org-string">'var'</span>] = result[<span class="org-string">'mean'</span>] + np.square(result[<span class="org-string">'mean'</span>]) * np.exp(result[<span class="org-string">'log_phi_hat'</span>])
  <span class="org-variable-name">log_cpm</span> = np.log(np.ma.masked_values(umi.reshape(num_trials, -1, umi.shape[-1]), 0)) - np.log(num_mols) + 6 * np.log(10)
  <span class="org-variable-name">result</span>[<span class="org-string">'mean_log_cpm'</span>] = log_cpm.mean(axis=1).ravel(order=<span class="org-string">'F'</span>)
  <span class="org-variable-name">result</span>[<span class="org-string">'var_log_cpm'</span>] = log_cpm.var(axis=1).ravel(order=<span class="org-string">'F'</span>)
  <span class="org-keyword">return</span> result

<span class="org-variable-name">res</span> = pd.concat([evaluate(num_samples, num_mols)
                 <span class="org-keyword">for</span> num_samples <span class="org-keyword">in</span> np.linspace(100, 1000, 5).astype(<span class="org-builtin">int</span>)
                 <span class="org-keyword">for</span> num_mols <span class="org-keyword">in</span> 1e3 * np.linspace(100, 1000, 5).astype(<span class="org-builtin">int</span>)])
<span class="org-keyword">with</span> sqlite3.connect(<span class="org-string">"/project2/mstephens/aksarkar/projects/singlecell-qtl/browser/browser2.db"</span>) <span class="org-keyword">as</span> conn:
  res.to_sql(<span class="org-string">'simulation'</span>, conn, index=<span class="org-constant">False</span>, if_exists=<span class="org-string">'replace'</span>)
</pre>
</div>

<div class="org-src-container">
<pre class="src src-sh">sbatch --partition=gpu --gres=gpu:1 --mem=16G --job-name=tf-sim --output=sim.out
<span class="org-comment-delimiter">#</span><span class="org-comment">!/bin/bash</span>
<span class="org-builtin">source</span> activate scqtl
python /project2/mstephens/aksarkar/projects/singlecell-qtl/code/tf-sim.py
</pre>
</div>

<pre class="example">
Submitted batch job 45713922

</pre>
</div>
</div>
</div>

<div id="outline-container-org6ace9b3" class="outline-2">
<h2 id="org6ace9b3">numpy/scipy implementation</h2>
<div class="outline-text-2" id="text-org6ace9b3">
<p>
Optimize the negative log-likelihood.
</p>

<div class="org-src-container">
<pre class="src src-ipython" id="org529a27d"><span class="org-keyword">def</span> <span class="org-function-name">log</span>(x):
  <span class="org-doc">"""Numerically safe log"""</span>
  <span class="org-keyword">return</span> np.log(x + 1e-8)

<span class="org-keyword">def</span> <span class="org-function-name">sigmoid</span>(x):
  <span class="org-doc">"""Numerically safe sigmoid"""</span>
  <span class="org-variable-name">lim</span> = np.log(np.finfo(np.float64).resolution)
  <span class="org-keyword">return</span> np.clip(sp.expit(x), lim, -lim)

<span class="org-keyword">def</span> <span class="org-function-name">nb</span>(theta, x, size, onehot, design):
  <span class="org-doc">"""Return the per-data point log likelihood</span>

<span class="org-doc">  x ~ Poisson(size .* design' * theta[2 * m:k] * exp(onehot * theta[:m]) * u)</span>
<span class="org-doc">  u ~ Gamma(exp(onehot * theta[m:2 * m]), exp(onehot * theta[m:2 * m]))</span>

<span class="org-doc">  theta - (2 * m + k, 1)</span>
<span class="org-doc">  x - (n, 1)</span>
<span class="org-doc">  size - (n, 1)</span>
<span class="org-doc">  onehot - (n, m)</span>
<span class="org-doc">  design - (n, k)</span>

<span class="org-doc">  """</span>
  <span class="org-variable-name">n</span>, <span class="org-variable-name">m</span> = onehot.shape
  <span class="org-keyword">assert</span> x.shape == (n,)
  <span class="org-keyword">assert</span> size.shape == (n,)
  <span class="org-keyword">assert</span> design.shape[0] == n
  <span class="org-keyword">assert</span> theta.shape == (2 * m + design.shape[1],)
  <span class="org-variable-name">mean</span> = size * np.exp(onehot.dot(theta[:m]) + design.dot(theta[2 * m:]))
  <span class="org-keyword">assert</span> mean.shape == (n,)
  <span class="org-variable-name">inv_disp</span> = onehot.dot(np.exp(theta[m:2 * m]))
  <span class="org-keyword">assert</span> inv_disp.shape == (n,)
  <span class="org-keyword">return</span> (x * log(mean / inv_disp) -
          x * log(1 + mean / inv_disp) -
          inv_disp * log(1 + mean / inv_disp) +
          sp.gammaln(x + inv_disp) -
          sp.gammaln(inv_disp) -
          sp.gammaln(x + 1))

<span class="org-keyword">def</span> <span class="org-function-name">_nb</span>(theta, x, size, onehot, design=<span class="org-constant">None</span>):
  <span class="org-doc">"""Return the mean negative log likelihood of x"""</span>
  <span class="org-keyword">return</span> -nb(theta, x, size, onehot, design).mean()

<span class="org-keyword">def</span> <span class="org-function-name">zinb</span>(theta, x, size, onehot, design=<span class="org-constant">None</span>):
  <span class="org-doc">"""Return the mean negative log likelihood of x"""</span>
  <span class="org-variable-name">n</span>, <span class="org-variable-name">m</span> = onehot.shape
  <span class="org-variable-name">logodds</span>, <span class="org-variable-name">theta</span> = theta[:m], theta[m:]
  <span class="org-variable-name">case_non_zero</span> = -np.log1p(np.exp(onehot.dot(logodds))) + nb(theta, x, size, onehot, design)
  <span class="org-variable-name">case_zero</span> = np.logaddexp(onehot.dot(logodds - np.log1p(np.exp(logodds))), case_non_zero)
  <span class="org-keyword">return</span> -np.where(x &lt; 1, case_zero, case_non_zero).mean()

<span class="org-keyword">def</span> <span class="org-function-name">_fit_gene</span>(chunk, onehot, design=<span class="org-constant">None</span>):
  <span class="org-variable-name">n</span>, <span class="org-variable-name">m</span> = onehot.shape
  <span class="org-keyword">assert</span> chunk.shape[0] == n
  <span class="org-comment-delimiter"># </span><span class="org-comment">We need to take care here to initialize mu=-inf for all zero observations</span>
  <span class="org-variable-name">x0</span> = np.log((onehot * chunk[:,:1]).<span class="org-builtin">sum</span>(axis=0) / onehot.<span class="org-builtin">sum</span>(axis=0)) - np.log(np.ma.masked_values(onehot, 0) * chunk[:,1:]).mean(axis=0).compressed()
  <span class="org-variable-name">x0</span> = np.hstack((x0, np.zeros(m)))
  <span class="org-keyword">if</span> design <span class="org-keyword">is</span> <span class="org-keyword">not</span> <span class="org-constant">None</span>:
    <span class="org-keyword">assert</span> design.shape[0] == n
    <span class="org-variable-name">design</span> -= design.mean(axis=0)
    <span class="org-variable-name">x0</span> = np.hstack((x0, np.zeros(design.shape[1])))
  <span class="org-variable-name">res0</span> = so.minimize(_nb, x0=x0, args=(chunk[:,0], chunk[:,1], onehot, design))
  <span class="org-variable-name">res</span> = so.minimize(zinb, x0=<span class="org-builtin">list</span>(np.zeros(m)) + <span class="org-builtin">list</span>(res0.x), args=(chunk[:,0], chunk[:,1], onehot, design))
  <span class="org-keyword">if</span> res0.fun &lt; res.fun:
    <span class="org-comment-delimiter"># </span><span class="org-comment">This isn't a likelihood ratio test. Numerically, our implementation of</span>
    <span class="org-comment-delimiter"># </span><span class="org-comment">ZINB can't represent pi = 0, so we need to use a separate implementation</span>
    <span class="org-comment-delimiter"># </span><span class="org-comment">for it</span>
    <span class="org-variable-name">log_mu</span> = res0.x[:m]
    <span class="org-variable-name">neg_log_phi</span> = res0.x[m:2 * m]
    <span class="org-variable-name">logit_pi</span> = np.zeros(m)
    logit_pi.fill(-np.inf)
  <span class="org-keyword">else</span>:
    <span class="org-variable-name">logit_pi</span> = res.x[:m]
    <span class="org-variable-name">log_mu</span> = res.x[m:2 * m]
    <span class="org-variable-name">neg_log_phi</span> = res.x[2 * m:3 * m]
  <span class="org-variable-name">mean_by_sample</span> = chunk[:,1] * onehot.dot(np.exp(log_mu))
  <span class="org-variable-name">var_by_sample</span> = mean_by_sample + np.square(mean_by_sample) * onehot.dot(np.exp(-neg_log_phi))
  <span class="org-variable-name">mean_by_ind</span> = np.ma.masked_equal(onehot * mean_by_sample.reshape(-1, 1), 0).mean(axis=0).filled(0)
  <span class="org-variable-name">var_by_ind</span> = np.ma.masked_equal(onehot * (np.square(mean_by_sample - onehot.dot(mean_by_ind)) + var_by_sample).reshape(-1, 1), 0).mean(axis=0).filled(0)
  <span class="org-keyword">return</span> [log_mu, -neg_log_phi, logit_pi, mean_by_ind, var_by_ind]

<span class="org-keyword">def</span> <span class="org-function-name">fit_gene</span>(chunk, bootstraps=100):
  <span class="org-variable-name">orig</span> = _fit_gene(chunk)
  <span class="org-variable-name">B</span> = []
  <span class="org-keyword">for</span> _ <span class="org-keyword">in</span> <span class="org-builtin">range</span>(bootstraps):
    B.append(_fit_gene(chunk[np.random.choice(chunk.shape[0], chunk.shape[0], replace=<span class="org-constant">True</span>)]))
  <span class="org-variable-name">se</span> = np.array(B)[:,:2].std(axis=0)
  <span class="org-keyword">return</span> orig + <span class="org-builtin">list</span>(se.ravel())
</pre>
</div>

<p>
Computing analytic SE runs into numerical problems.
</p>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-keyword">def</span> <span class="org-function-name">_pois</span>(theta, x, size):
  <span class="org-variable-name">mean</span> = np.exp(theta)
  <span class="org-variable-name">mean</span> *= size
  <span class="org-keyword">return</span> (x * log(mean) - mean - sp.gammaln(x + 1)).mean()

<span class="org-keyword">def</span> <span class="org-function-name">_pois_jac</span>(theta, x, size):
  <span class="org-variable-name">mean</span> = np.exp(theta)
  <span class="org-keyword">return</span> mean * (x / mean - size).mean()

<span class="org-keyword">def</span> <span class="org-function-name">_nb_jac</span>(theta, x, size):
  <span class="org-variable-name">mean</span>, <span class="org-variable-name">inv_disp</span> = np.exp(theta)
  <span class="org-variable-name">T</span> = (1 + size * mean / inv_disp)
  <span class="org-keyword">return</span> mean * (x / mean - size / inv_disp * (x + inv_disp) / T).mean()

<span class="org-keyword">def</span> <span class="org-function-name">check_gradients</span>(x, f, df, args=<span class="org-constant">None</span>, num_trials=100):
  <span class="org-variable-name">x</span> = np.array(x)
  <span class="org-variable-name">y</span> = f(x, *args)
  <span class="org-variable-name">analytic_diff</span> = df(x, *args)
  <span class="org-variable-name">error</span> = []
  <span class="org-keyword">for</span> i <span class="org-keyword">in</span> <span class="org-builtin">range</span>(num_trials):
    <span class="org-variable-name">eps</span> = np.random.normal(scale=1e-4, size=x.shape)
    <span class="org-variable-name">num_diff</span> = (f(x + eps, *args) - y) / eps
    error.append(<span class="org-builtin">abs</span>(num_diff - analytic_diff))
  <span class="org-keyword">return</span> np.array(error)
</pre>
</div>
</div>

<div id="outline-container-org1b75a70" class="outline-3">
<h3 id="org1b75a70">Simulation</h3>
<div class="outline-text-3" id="text-org1b75a70">
<p>
Check the parameter estimation on simulated data.
</p>

<p>
Assuming simulated confounders \(x\) are isotropic Gaussian, we can derive
the scale of \(\beta\) to achieve a specified fold-change in relative
abundance:
</p>

<p>
\[ x \sim N(0, 1) \]
</p>

<p>
Letting \(\tau\) denote precision:
</p>

<p>
\[ \beta \sim N(0, \tau) \]
</p>

<p>
\[ x\beta \sim N(0, 1 + \tau) \]
</p>

<p>
\[ \mathbb{E}[x\beta] = y = \exp\left(\frac{1}{2 (1 + \tau)}\right) \]
</p>

<p>
\[ \tau = \frac{1 - 2 \ln y}{2 \ln y} \]  
</p>

<div class="org-src-container">
<pre class="src src-ipython" id="orgd2a03c7"><span class="org-keyword">def</span> <span class="org-function-name">simulate</span>(num_samples, size=<span class="org-constant">None</span>, log_mu=<span class="org-constant">None</span>, log_phi=<span class="org-constant">None</span>, logodds=<span class="org-constant">None</span>, seed=<span class="org-constant">None</span>, design=<span class="org-constant">None</span>, fold=<span class="org-constant">None</span>):
  <span class="org-keyword">if</span> seed <span class="org-keyword">is</span> <span class="org-constant">None</span>:
    <span class="org-variable-name">seed</span> = 0
  np.random.seed(seed)
  <span class="org-keyword">if</span> log_mu <span class="org-keyword">is</span> <span class="org-constant">None</span>:
    <span class="org-variable-name">log_mu</span> = np.random.uniform(low=-12, high=-8)
  <span class="org-keyword">if</span> log_phi <span class="org-keyword">is</span> <span class="org-constant">None</span>:
    <span class="org-variable-name">log_phi</span> = np.random.uniform(low=-6, high=0)
  <span class="org-keyword">if</span> size <span class="org-keyword">is</span> <span class="org-constant">None</span>:
    <span class="org-variable-name">size</span> = 1e5
  <span class="org-keyword">if</span> logodds <span class="org-keyword">is</span> <span class="org-constant">None</span>:
    <span class="org-variable-name">prob</span> = np.random.uniform()
  <span class="org-keyword">else</span>:
    <span class="org-variable-name">prob</span> = sp.expit(logodds)
  <span class="org-keyword">if</span> design <span class="org-keyword">is</span> <span class="org-constant">None</span>:
    <span class="org-variable-name">design</span> = np.random.normal(size=(num_samples, 1))
  <span class="org-keyword">else</span>:
    <span class="org-keyword">assert</span> design.shape[0] == num_samples
  <span class="org-keyword">if</span> fold <span class="org-keyword">is</span> <span class="org-constant">None</span> <span class="org-keyword">or</span> np.isclose(fold, 1):
    <span class="org-variable-name">beta</span> = np.array([[0]])
  <span class="org-keyword">else</span>:
    <span class="org-keyword">assert</span> fold &gt; 1
    <span class="org-variable-name">beta</span> = np.random.normal(size=(design.shape[1], 1), scale=2 * np.log(fold) / (1 - 2 * np.log(fold)))

  <span class="org-variable-name">n</span> = np.exp(-log_phi)
  <span class="org-variable-name">p</span> = 1 / (1 + size * np.exp(log_mu + design.dot(beta) + log_phi)).ravel()
  <span class="org-variable-name">x</span> = np.where(np.random.uniform(size=num_samples) &lt; prob,
               0,
               np.random.negative_binomial(n=n, p=p, size=num_samples))
  <span class="org-keyword">return</span> np.vstack((x, size * np.ones(num_samples))).T, design

<span class="org-keyword">def</span> <span class="org-function-name">batch_design_matrix</span>(num_samples, num_batches):
  <span class="org-doc">"""Return a matrix of binary indicators representing batch assignment"""</span>
  <span class="org-variable-name">design</span> = np.zeros((num_samples, num_batches))
  <span class="org-variable-name">design</span>[np.arange(num_samples), np.random.choice(num_batches, size=num_samples)] = 1
  <span class="org-keyword">return</span> design

<span class="org-keyword">def</span> <span class="org-function-name">evaluate</span>(num_samples, num_mols, log_mu, log_phi, logodds, fold, trial):
  <span class="org-variable-name">x</span>, <span class="org-variable-name">design</span> = simulate(num_samples=num_samples, size=num_mols,
                       log_mu=log_mu, log_phi=log_phi,
                       logodds=logodds, design=<span class="org-constant">None</span>, fold=fold, seed=trial)
  <span class="org-variable-name">onehot</span> = np.ones((num_samples, 1))
  <span class="org-variable-name">keys</span> = [<span class="org-string">'num_samples'</span>, <span class="org-string">'num_mols'</span>, <span class="org-string">'log_mu'</span>, <span class="org-string">'log_phi'</span>, <span class="org-string">'logodds'</span>, <span class="org-string">'trial'</span>,
          <span class="org-string">'fold'</span>, <span class="org-string">'log_mu_hat'</span>, <span class="org-string">'log_phi_hat'</span>, <span class="org-string">'logodds_hat'</span>, <span class="org-string">'mean'</span>, <span class="org-string">'var'</span>]
  <span class="org-variable-name">result</span> = [num_samples, num_mols, log_mu, log_phi, logodds, trial, fold] + [param[0] <span class="org-keyword">for</span> param <span class="org-keyword">in</span> _fit_gene(x, onehot, design)]
  <span class="org-variable-name">result</span> = {k: v <span class="org-keyword">for</span> k, v <span class="org-keyword">in</span> <span class="org-builtin">zip</span>(keys, result)}
  <span class="org-variable-name">eps</span> = .5 / num_mols
  <span class="org-variable-name">log_cpm</span> = (np.log(np.ma.masked_values(x[:,0], 0) + eps) - np.log(x[:,1] + 2 * eps) + 6 * np.log(10)).compressed()
  <span class="org-variable-name">result</span>[<span class="org-string">'mean_log_cpm'</span>] = log_cpm.mean()
  <span class="org-variable-name">result</span>[<span class="org-string">'var_log_cpm'</span>] = log_cpm.var()
  <span class="org-keyword">return</span> result
</pre>
</div>

<p>
Check the implementation actually worked.
</p>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-variable-name">x1</span>, <span class="org-variable-name">design1</span> = simulate(num_samples=1000, size=1e5, log_mu=-8, log_phi=-6, logodds=-3, seed=0, design=batch_design_matrix(1000, 2), fold=1.1)
<span class="org-variable-name">x2</span>, <span class="org-variable-name">design2</span> = simulate(num_samples=1000, size=1e5, log_mu=-9, log_phi=-6, logodds=-3, seed=0, design=batch_design_matrix(1000, 2), fold=1.1)
</pre>
</div>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-variable-name">x</span> = np.vstack((x1, x2))
<span class="org-variable-name">design</span> = np.vstack((design1, design2))
<span class="org-variable-name">onehot</span> = np.zeros((2000, 2))
<span class="org-variable-name">onehot</span>[:1000,0] = 1
<span class="org-variable-name">onehot</span>[1000:,1] = 1
</pre>
</div>

<div class="org-src-container">
<pre class="src src-ipython">so.minimize(_nb, np.zeros(6), (x[:,0], x[:,1], onehot, design - design.mean(axis=0)))
</pre>
</div>

<pre class="example">
fun: 3.7693708861600173
hess_inv: array([[ 2.95492914e-01, -2.26820271e-02, -6.88886930e-02,
3.66568258e-02, -1.35008739e-02,  1.33990791e-02],
[-2.26820271e-02,  2.76704513e-01,  4.55879587e-03,
6.39326308e-02, -2.03557128e-02,  2.03675939e-02],
[-6.88886930e-02,  4.55879587e-03,  8.40491928e+00,
3.47009715e-01,  1.14991770e-02, -1.17004131e-02],
[ 3.66568258e-02,  6.39326308e-02,  3.47009715e-01,
1.93344347e+01, -8.18164095e-03,  6.19982324e-03],
[-1.35008739e-02, -2.03557128e-02,  1.14991770e-02,
-8.18164095e-03,  6.45950264e-01,  3.54059802e-01],
[ 1.33990791e-02,  2.03675939e-02, -1.17004131e-02,
6.19982324e-03,  3.54059802e-01,  6.45930363e-01]])
jac: array([ 6.82473183e-06,  8.34465027e-06,  1.07288361e-06,  8.34465027e-07,
3.75509262e-06, -4.08291817e-06])
message: 'Optimization terminated successfully.'
nfev: 280
nit: 30
njev: 35
status: 0
success: True
x: array([-7.79891405, -8.79063513,  2.05714587,  2.56621654,  0.16492975,
-0.16494031])
</pre>

<div class="org-src-container">
<pre class="src src-ipython">so.minimize(zinb, np.zeros(8), (x[:,0], x[:,1], onehot, design - design.mean(axis=0)))
</pre>
</div>

<pre class="example">
fun: 3.1120455141161147
hess_inv: array([[ 3.91509877e+01, -6.82255452e+00,  3.56651927e-03,
-3.60544664e-03,  1.89769061e-03, -3.67396633e-05,
-5.05335375e-03,  2.07017842e-03],
[-6.82255452e+00,  3.63677578e+01, -2.89665006e-03,
1.89766982e-03, -1.15515147e-03,  1.19995032e-04,
4.03751660e-03, -1.28881738e-03],
[ 3.56651927e-03, -2.89665006e-03,  1.00060484e-05,
-4.75805864e-06, -1.52866758e-06, -9.78212511e-07,
-7.93108063e-09, -9.33841063e-06],
[-3.60544664e-03,  1.89766982e-03, -4.75805864e-06,
9.43027846e-06, -1.92946942e-08, -1.76906831e-07,
-3.04179309e-07,  5.35291550e-06],
[ 1.89769061e-03, -1.15515147e-03, -1.52866758e-06,
-1.92946942e-08,  2.96476652e-06,  1.16116940e-06,
-4.91977746e-06,  2.37359794e-06],
[-3.67396633e-05,  1.19995032e-04, -9.78212511e-07,
-1.76906831e-07,  1.16116940e-06,  1.48628846e-06,
-1.36236523e-06,  7.97280863e-07],
[-5.05335375e-03,  4.03751660e-03, -7.93108063e-09,
-3.04179309e-07, -4.91977746e-06, -1.36236523e-06,
1.60495775e-05, -3.42044027e-06],
[ 2.07017842e-03, -1.28881738e-03, -9.33841063e-06,
5.35291550e-06,  2.37359794e-06,  7.97280863e-07,
-3.42044027e-06,  1.96937195e-05]])
jac: array([-2.98023224e-07,  7.15255737e-07,  9.99987125e-04, -3.08364630e-04,
5.21874428e-03,  7.71874189e-03,  1.99797750e-03,  9.57250595e-05])
message: 'Desired error not necessarily achieved due to precision loss.'
nfev: 1942
nit: 80
njev: 193
status: 2
success: False
x: array([-3.07858135, -3.07854889, -7.75387174, -8.74631249, 12.91019024,
13.26839552,  0.04538458, -0.28914969])
</pre>

<div class="org-src-container">
<pre class="src src-ipython">_fit_gene(x, onehot)
</pre>
</div>

<pre class="example">
[array([-7.74001987, -8.73200364]),
array([-3.50778747, -3.59795896]),
array([-3.07854886, -3.07859407]),
array([43.50629319, 16.13388663]),
array([100.22044275,  23.26084595])]
</pre>

<div class="org-src-container">
<pre class="src src-ipython">_fit_gene(x, onehot, design)
</pre>
</div>

<pre class="example">
[array([-7.74986202, -8.7418084 ]),
array([-6.82644387, -6.09474886]),
array([-3.0785687 , -3.07861418]),
array([43.08019804, 15.97647082]),
array([45.09331255, 16.55197158])]
</pre>

<p>
Check what happens on all zero data.
</p>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-variable-name">x</span> = np.concatenate((np.zeros((1000, 1)), 1e5 * np.ones((1000, 1))), axis=1)
<span class="org-variable-name">onehot</span> = np.ones((1000, 1))
</pre>
</div>

<div class="org-src-container">
<pre class="src src-ipython">np.log((onehot * x[:,:1]).<span class="org-builtin">sum</span>(axis=0) / onehot.<span class="org-builtin">sum</span>(axis=0)) - np.log(np.ma.masked_values(onehot, 0) * x[:,1:]).mean(axis=0).compressed()
</pre>
</div>

<pre class="example">
array([-inf])

</pre>

<div class="org-src-container">
<pre class="src src-ipython">so.minimize(_nb, x0=(-np.inf, 0), args=(x[:,0], x[:,1], onehot))
</pre>
</div>

<pre class="example">
fun: 9.999999889225288e-09
hess_inv: array([[1, 0],
[0, 1]])
jac: array([0.00000000e+00, 1.00000003e-08])
message: 'Optimization terminated successfully.'
nfev: 4
nit: 0
njev: 1
status: 0
success: True
x: array([-inf,   0.])
</pre>

<div class="org-src-container">
<pre class="src src-ipython">_fit_gene(x, onehot)
</pre>
</div>

<pre class="example">
[array([-inf]), array([-0.]), array([0.]), array([0.]), array([0.])]

</pre>

<div class="org-src-container">
<pre class="src src-ipython">_fit_gene(x, onehot, design)
</pre>
</div>

<pre class="example">
[array([-7.75254245, -8.74529525]),
array([-7.38635243, -6.45950176]),
array([-3.07849171, -3.07866706]),
array([42.9648789 , 15.92086032]),
array([44.10874472, 16.3176927 ])]
</pre>

<p>
Check the end-to-end evaluation.
</p>

<div class="org-src-container">
<pre class="src src-ipython">evaluate(num_samples=100, num_mols=1e5, log_mu=-8, log_phi=-6, logodds=-3, fold=1.1, trial=0)
</pre>
</div>

<pre class="example">
{'fold': 1.1,
'log_mu': -8,
'log_mu_hat': -7.980952099176061,
'log_phi': -6,
'log_phi_hat': -5.681342153408702,
'logodds': -3,
'logodds_hat': -2.7515411255776483,
'mean': 34.19137317124226,
'mean_log_cpm': 5.8224834413816415,
'num_mols': 100000.0,
'num_samples': 100,
'trial': 0,
'var': 38.1766412350187,
'var_log_cpm': 0.23075096605184894}
</pre>

<div class="org-src-container">
<pre class="src src-ipython">%timeit evaluate(num_samples=5000, num_mols=1e5, log_mu=-8, log_phi=-6, logodds=-3, fold=1.1, trial=0)
</pre>
</div>

<p>
1.41 s  449 s per loop (mean  std. dev. of 7 runs, 1 loop each)
</p>

<p>
Investigate what happens as the number of confounders increases.
</p>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-variable-name">design</span> = np.random.normal(size=(300, 20))
<span class="org-variable-name">x</span>, <span class="org-variable-name">_</span> = simulate(num_samples=300, size=1e5, log_mu=-8, log_phi=-6, logodds=-3, seed=0, design=design, fold=1.1)
_fit_gene(x, design)
</pre>
</div>

<pre class="example">
[-8.00649125343069,
-6.410804905890429,
-2.99891159295469,
33.32921072894424,
35.15509338485383]
</pre>

<p>
Run the simulation on 28 CPUs.
</p>

<div class="org-src-container">
<pre class="src src-ipython">&lt;&lt;zinb-imports&gt;&gt;
<span class="org-keyword">import</span> multiprocessing <span class="org-keyword">as</span> mp
<span class="org-keyword">import</span> sqlite3
&lt;&lt;np-zinb-impl2&gt;&gt;
&lt;&lt;sim-impl2&gt;&gt;
<span class="org-variable-name">args</span> = [(num_samples, num_mols, log_mu, log_phi, logodds, fold, trial)
        <span class="org-keyword">for</span> num_samples <span class="org-keyword">in</span> np.linspace(100, 1000, 5).astype(<span class="org-builtin">int</span>)
        <span class="org-keyword">for</span> num_mols <span class="org-keyword">in</span> 1e3 * np.linspace(100, 1000, 5).astype(<span class="org-builtin">int</span>)
        <span class="org-keyword">for</span> log_mu <span class="org-keyword">in</span> np.linspace(-12, -6, 7)
        <span class="org-keyword">for</span> log_phi <span class="org-keyword">in</span> np.linspace(-6, 0, 7)
        <span class="org-keyword">for</span> logodds <span class="org-keyword">in</span> np.linspace(-3, 3, 7)
        <span class="org-keyword">for</span> fold <span class="org-keyword">in</span> np.linspace(1, 1.25, 6)
        <span class="org-keyword">for</span> trial <span class="org-keyword">in</span> <span class="org-builtin">range</span>(10)]
<span class="org-keyword">with</span> mp.Pool() <span class="org-keyword">as</span> pool:
  <span class="org-variable-name">result</span> = pd.DataFrame.from_dict(pool.starmap(evaluate, args))
<span class="org-keyword">with</span> sqlite3.connect(<span class="org-string">'/project2/mstephens/aksarkar/projects/singlecell-qtl/browser/browser.db'</span>) <span class="org-keyword">as</span> conn:
  result.to_sql(name=<span class="org-string">'simulation'</span>, con=conn, index=<span class="org-constant">False</span>, if_exists=<span class="org-string">'replace'</span>)
  conn.execute(<span class="org-string">'create index ix_simulation on simulation(num_samples, num_mols);'</span>)
</pre>
</div>

<div class="org-src-container">
<pre class="src src-sh">sbatch --partition=broadwl --mem=8G --job-name sim -n1 -c28 --exclusive --out sim.out
<span class="org-comment-delimiter">#</span><span class="org-comment">!/bin/bash</span>
<span class="org-builtin">source</span> activate scqtl
python /project2/mstephens/aksarkar/projects/singlecell-qtl/code/sim.py
</pre>
</div>

<pre class="example">
Submitted batch job 45559291

</pre>

<p>
Use this to check the parameter estimation for a particular gene/individual.
</p>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-keyword">def</span> <span class="org-function-name">extract_data</span>(ind, gene):
  <span class="org-keyword">with</span> sqlite3.connect(<span class="org-string">'/project2/mstephens/aksarkar/projects/singlecell-qtl/browser/browser.db'</span>) <span class="org-keyword">as</span> conn:
    <span class="org-variable-name">umi</span> = pd.read_sql(<span class="org-string">"""select umi.value, annotation.size from umi, annotation </span>
<span class="org-string">    where annotation.chip_id == ? and gene == ? and </span>
<span class="org-string">    umi.sample == annotation.sample;"""</span>, con=conn, params=(ind, gene))
    <span class="org-keyword">return</span> umi
</pre>
</div>

<p>
Shard the data to parallelize over nodes. During this pass, write the data
out to the database. 
</p>

<p>
<b>Important notes:</b> 
</p>

<ol class="org-ol">
<li>We need to use the actual number of molecules detected as the size factor,
not the sum of QC'ed counts</li>
<li>We need to estimate relative abundance for all genes (to match bulk), not
just those which passed QC</li>
<li>We need to recode categorical covariates as binary indicators. Although
over the entire data set the number of indicators (in particular,
<code>experiment</code>) might be larger than the number of observations, for each
subproblem it will not be.</li>
</ol>

<p>
Write the annotations to the database.
</p>

<div class="org-src-container">
<pre class="src src-ipython" id="orgda7fb7e"><span class="org-variable-name">keep_samples</span> = pd.read_table(<span class="org-string">'/project2/mstephens/aksarkar/projects/singlecell-qtl/data/quality-single-cells.txt'</span>, index_col=0, header=<span class="org-constant">None</span>)
<span class="org-variable-name">annotations</span> = pd.read_table(<span class="org-string">'/project2/mstephens/aksarkar/projects/singlecell-qtl/data/scqtl-annotation.txt'</span>)
<span class="org-variable-name">annotations</span>[<span class="org-string">'sample'</span>] = annotations.<span class="org-builtin">apply</span>(<span class="org-keyword">lambda</span> x: <span class="org-string">'{chip_id}.{experiment:08d}.{well}'</span>.<span class="org-builtin">format</span>(**<span class="org-builtin">dict</span>(x)), axis=1)
<span class="org-variable-name">annotations</span>[<span class="org-string">'size'</span>] = annotations[<span class="org-string">'mol_hs'</span>]
<span class="org-variable-name">annotations</span> = annotations.loc[keep_samples.values.ravel(), [<span class="org-string">'sample'</span>, <span class="org-string">'chip_id'</span>, <span class="org-string">'size'</span>]]
<span class="org-keyword">with</span> sqlite3.connect(<span class="org-string">'/project2/mstephens/aksarkar/projects/singlecell-qtl/browser/browser.db'</span>) <span class="org-keyword">as</span> conn:
  annotations.to_sql(name=<span class="org-string">'annotation'</span>, con=conn, if_exists=<span class="org-string">'replace'</span>)
  conn.execute(<span class="org-string">'create index ix_annotation on annotation(chip_id, sample);'</span>)
</pre>
</div>

<p>
Recode categorical variables as binary indicators.
</p>

<div class="org-src-container">
<pre class="src src-ipython" id="orgf00e99d"><span class="org-keyword">def</span> <span class="org-function-name">recode</span>(annotations, key):
  <span class="org-variable-name">n</span> = annotations.shape[0]
  <span class="org-variable-name">cat</span> = <span class="org-builtin">sorted</span>(<span class="org-builtin">set</span>(annotations[key]))
  <span class="org-variable-name">onehot</span> = np.zeros((n, <span class="org-builtin">len</span>(cat)))
  onehot[np.arange(n), annotations[key].<span class="org-builtin">apply</span>(cat.index)] = 1
  <span class="org-keyword">return</span> onehot
</pre>
</div>

<p>
Check that we will have enough data points to actually estimate effects:
</p>

<div class="org-src-container">
<pre class="src src-ipython">annotations[keep_samples.values.ravel()].groupby(<span class="org-string">'chip_id'</span>).<span class="org-builtin">apply</span>(<span class="org-keyword">lambda</span> x: pd.Series(recode(x, <span class="org-string">'experiment'</span>).shape)).describe()
</pre>
</div>

<pre class="example">
0          1
count   54.000000  54.000000
mean    96.685185   5.500000
std     40.056952   1.969101
min     19.000000   2.000000
25%     78.000000   4.000000
50%     88.500000   5.000000
75%    102.250000   6.750000
max    281.000000  14.000000
</pre>

<p>
Show that <code>batch</code> is a linear combination of <code>experiment</code>.
</p>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-variable-name">design</span> = np.concatenate([recode(annotations, k) <span class="org-keyword">for</span> k <span class="org-keyword">in</span> (<span class="org-string">'batch'</span>, <span class="org-string">'experiment'</span>)], axis=1)
<span class="org-variable-name">u</span>, <span class="org-variable-name">d</span>, <span class="org-variable-name">v</span> = np.linalg.svd(design)
design.shape[1] - (d &gt; 1e-2).<span class="org-builtin">sum</span>()
</pre>
</div>

<pre class="example">
6

</pre>


<div class="org-src-container">
<pre class="src src-ipython">&lt;&lt;zinb-imports&gt;&gt;
&lt;&lt;write-annotation-impl&gt;&gt;
<span class="org-variable-name">i</span> = 0
<span class="org-keyword">with</span> sqlite3.connect(<span class="org-string">'/project2/mstephens/aksarkar/projects/singlecell-qtl/browser/browser.db'</span>) <span class="org-keyword">as</span> conn:
  conn.execute(<span class="org-string">'drop table if exists umi;'</span>)
  <span class="org-keyword">for</span> chunk <span class="org-keyword">in</span> pd.read_table(<span class="org-string">'/project2/mstephens/aksarkar/projects/singlecell-qtl/data/scqtl-counts.txt.gz'</span>, index_col=0, chunksize=1000):
    <span class="org-keyword">print</span>(<span class="org-string">'Processing chunk {}'</span>.<span class="org-builtin">format</span>(i))
    <span class="org-variable-name">chunk</span> = (chunk
             .loc[:,keep_samples.values.ravel()])
    <span class="org-keyword">if</span> <span class="org-keyword">not</span> chunk.empty:
      <span class="org-variable-name">chunk</span> = (chunk
               .reset_index()
               .melt(id_vars=<span class="org-string">'gene'</span>, var_name=<span class="org-string">'sample'</span>)
               .merge(annotations, on=<span class="org-string">'sample'</span>)
               .sort_values([<span class="org-string">'gene'</span>, <span class="org-string">'chip_id'</span>, <span class="org-string">'sample'</span>]))
      chunk.to_csv(<span class="org-string">'/scratch/midway2/aksarkar/singlecell/density-estimation/chunk-{}.txt.gz'</span>.<span class="org-builtin">format</span>(i), columns=[<span class="org-string">'gene'</span>, <span class="org-string">'chip_id'</span>, <span class="org-string">'sample'</span>, <span class="org-string">'value'</span>, <span class="org-string">'size'</span>], compression=<span class="org-string">'gzip'</span>, sep=<span class="org-string">'\t'</span>)
      chunk[[<span class="org-string">'gene'</span>, <span class="org-string">'sample'</span>, <span class="org-string">'value'</span>]].to_sql(name=<span class="org-string">'umi'</span>, con=conn, index=<span class="org-constant">False</span>, if_exists=<span class="org-string">'append'</span>)
      <span class="org-variable-name">i</span> += 1
    <span class="org-keyword">del</span> chunk
  conn.execute(<span class="org-string">'create index ix_umi on umi(gene, sample);'</span>)
</pre>
</div>

<div class="org-src-container">
<pre class="src src-sh">sbatch --partition=broadwl --mem=8G --job-name shard --out shard.out
<span class="org-comment-delimiter">#</span><span class="org-comment">!/bin/bash</span>
<span class="org-builtin">source</span> activate scqtl
python /project2/mstephens/aksarkar/projects/singlecell-qtl/code/shard.py
</pre>
</div>

<pre class="example">
Submitted batch job 45561618

</pre>

<p>
Process each chunk in parallel.
</p>

<div class="org-src-container">
<pre class="src src-ipython" id="orgbd92003"><span class="org-keyword">def</span> <span class="org-function-name">compute_breaks</span>(chunk, by_ind=<span class="org-constant">False</span>):
  <span class="org-comment-delimiter"># </span><span class="org-comment">Each subproblem has fixed size, so we can just split on integer indices</span>
  <span class="org-comment-delimiter"># </span><span class="org-comment">(instead of grouping)</span>
  <span class="org-variable-name">num_genes</span> = <span class="org-builtin">len</span>(<span class="org-builtin">set</span>(chunk[<span class="org-string">'gene'</span>]))
  <span class="org-variable-name">num_samples</span> = <span class="org-builtin">len</span>(<span class="org-builtin">set</span>(chunk[<span class="org-string">'sample'</span>]))
  <span class="org-variable-name">breaks</span> = num_samples * np.arange(num_genes).reshape(-1, 1)
  <span class="org-keyword">if</span> by_ind:
    <span class="org-variable-name">num_samples_per_ind</span> = chunk.iloc[:num_samples][<span class="org-string">'chip_id'</span>].value_counts().sort_index().values
    <span class="org-comment-delimiter"># </span><span class="org-comment">This can't be written += because of broadcasting</span>
    <span class="org-variable-name">breaks</span> = breaks + np.cumsum(num_samples_per_ind).reshape(1, -1)
  <span class="org-keyword">else</span>:
    <span class="org-comment-delimiter"># </span><span class="org-comment">We need to get the right end point of each subproblem (exclusive)</span>
    <span class="org-variable-name">breaks</span> += num_samples
  <span class="org-keyword">return</span> breaks.ravel()
</pre>
</div>

<div class="org-src-container">
<pre class="src src-ipython">&lt;&lt;zinb-imports&gt;&gt;
<span class="org-keyword">import</span> argparse
<span class="org-keyword">import</span> gzip
<span class="org-keyword">import</span> os
<span class="org-keyword">import</span> multiprocessing <span class="org-keyword">as</span> mp
<span class="org-keyword">import</span> sqlite3
&lt;&lt;np-zinb-impl2&gt;&gt;
&lt;&lt;recode-impl&gt;&gt;
&lt;&lt;process-chunk-impl&gt;&gt;

<span class="org-variable-name">keep_samples</span> = pd.read_table(<span class="org-string">'/project2/mstephens/aksarkar/projects/singlecell-qtl/data/quality-single-cells.txt'</span>, index_col=0, header=<span class="org-constant">None</span>)
<span class="org-variable-name">annotations</span> = pd.read_table(<span class="org-string">'/project2/mstephens/aksarkar/projects/singlecell-qtl/data/scqtl-annotation.txt'</span>)
<span class="org-variable-name">annotations</span> = annotations[keep_samples.values.ravel()]

<span class="org-variable-name">onehot</span> = recode(annotations, <span class="org-string">'chip_id'</span>)
<span class="org-variable-name">design</span> = np.concatenate([recode(annotations, k) <span class="org-keyword">for</span> k <span class="org-keyword">in</span> (<span class="org-string">'batch'</span>, <span class="org-string">'experiment'</span>)], axis=1)

<span class="org-keyword">with</span> mp.Pool() <span class="org-keyword">as</span> pool:
  <span class="org-variable-name">chunk</span> = pd.read_table(<span class="org-string">'/scratch/midway2/aksarkar/singlecell/density-estimation/chunk-{}.txt.gz'</span>.<span class="org-builtin">format</span>(os.getenv(<span class="org-string">'SLURM_ARRAY_TASK_ID'</span>)))
  <span class="org-variable-name">breaks</span> = compute_breaks(chunk)
  <span class="org-variable-name">res</span> = pool.starmap(
    _fit_gene,
    <span class="org-builtin">zip</span>(np.split(chunk[[<span class="org-string">'value'</span>, <span class="org-string">'size'</span>]].values, breaks[:-1]),
        (onehot <span class="org-keyword">for</span> b <span class="org-keyword">in</span> breaks),
        (design <span class="org-keyword">for</span> b <span class="org-keyword">in</span> breaks)))

<span class="org-keyword">with</span> gzip.<span class="org-builtin">open</span>(<span class="org-string">'/scratch/midway2/aksarkar/singlecell/density-estimation/result-{}.txt.gz'</span>.<span class="org-builtin">format</span>(os.getenv(<span class="org-string">'SLURM_ARRAY_TASK_ID'</span>)), <span class="org-string">'wt'</span>) <span class="org-keyword">as</span> f:
  <span class="org-keyword">for</span> b, est <span class="org-keyword">in</span> <span class="org-builtin">zip</span>(breaks, res):
    <span class="org-variable-name">gene</span> = chunk.iloc[b - 1][<span class="org-string">'gene'</span>]
    <span class="org-keyword">for</span> ind, record <span class="org-keyword">in</span> <span class="org-builtin">zip</span>(<span class="org-builtin">sorted</span>(<span class="org-builtin">set</span>(annotations[<span class="org-string">'chip_id'</span>])), <span class="org-builtin">zip</span>(*est)):
      <span class="org-keyword">print</span>(gene, ind, *record, sep=<span class="org-string">'\t'</span>, <span class="org-builtin">file</span>=f)
</pre>
</div>

<div class="org-src-container">
<pre class="src src-sh">sbatch --partition=broadwl --job-name=<span class="org-string">"np-zinb"</span> --mem=8G -a 0-20 -n1 -c28 --exclusive
<span class="org-comment-delimiter">#</span><span class="org-comment">!/bin/bash</span>
<span class="org-builtin">source</span> activate scqtl
python /project2/mstephens/aksarkar/projects/singlecell-qtl/code/zinb.py
</pre>
</div>

<pre class="example">
Submitted batch job 45588176

</pre>

<p>
Populate the database.
</p>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-keyword">with</span> sqlite3.connect(<span class="org-string">'/project2/mstephens/aksarkar/projects/singlecell-qtl/browser/browser.db'</span>) <span class="org-keyword">as</span> conn:
  conn.execute(<span class="org-string">'drop table if exists params;'</span>)
  <span class="org-keyword">for</span> i <span class="org-keyword">in</span> <span class="org-builtin">range</span>(21):
    <span class="org-keyword">for</span> chunk <span class="org-keyword">in</span> pd.read_table(<span class="org-string">'/scratch/midway2/aksarkar/singlecell/density-estimation/result-{}.txt.gz'</span>.<span class="org-builtin">format</span>(i), sep=<span class="org-string">' '</span>, header=<span class="org-constant">None</span>, chunksize=1000):
      <span class="org-variable-name">chunk.columns</span> = [<span class="org-string">'gene'</span>, <span class="org-string">'ind'</span>, <span class="org-string">'log_mu'</span>, <span class="org-string">'log_phi'</span>, <span class="org-string">'logodds'</span>, <span class="org-string">'mean'</span>, <span class="org-string">'var'</span>, <span class="org-string">'log_mu_se'</span>, <span class="org-string">'log_phi_se'</span>]
      chunk.to_sql(name=<span class="org-string">'params'</span>, con=conn, index=<span class="org-constant">False</span>, if_exists=<span class="org-string">'append'</span>)
  conn.execute(<span class="org-string">'create index ix_params on params(gene, ind);'</span>)
</pre>
</div>
</div>
</div>
</div>

<div id="outline-container-org4f5c185" class="outline-2">
<h2 id="org4f5c185">Parameter distributions</h2>
<div class="outline-text-2" id="text-org4f5c185">
<p>
The simulation reveals the method has undesirable behavior when the
proportion of zeros is too large and mean is too small.
</p>

<p>
Read the estimated parameters.
</p>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-variable-name">log_mu</span> = pd.read_table(<span class="org-string">'/project2/mstephens/aksarkar/projects/singlecell-qtl/data/density-estimation/without-cell-cycle/zi2-log-mu.txt.gz'</span>, index_col=0, sep=<span class="org-string">' '</span>)
<span class="org-variable-name">log_phi</span> = pd.read_table(<span class="org-string">'/project2/mstephens/aksarkar/projects/singlecell-qtl/data/density-estimation/without-cell-cycle/zi2-log-phi.txt.gz'</span>, index_col=0, sep=<span class="org-string">' '</span>)
<span class="org-variable-name">logodds</span> = pd.read_table(<span class="org-string">'/project2/mstephens/aksarkar/projects/singlecell-qtl/data/density-estimation/without-cell-cycle/zi2-logodds.txt.gz'</span>, sep=<span class="org-string">' '</span>, index_col=0)
</pre>
</div>

<p>
Look at the joint distribution.
</p>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-variable-name">J</span> = (log_mu.agg(np.mean, axis=1).to_frame()
     .merge(log_phi.agg(np.mean, axis=1).to_frame(), left_index=<span class="org-constant">True</span>, right_index=<span class="org-constant">True</span>)
     .rename(columns={<span class="org-string">'0_x'</span>: <span class="org-string">'log_mu'</span>, <span class="org-string">'0_y'</span>: <span class="org-string">'log_phi'</span>})
     .merge(logodds.agg(np.mean, axis=1).to_frame(), left_index=<span class="org-constant">True</span>, right_index=<span class="org-constant">True</span>)
     .rename(columns={0: <span class="org-string">'logodds'</span>}))
J.head()
</pre>
</div>

<pre class="example">
log_mu   log_phi   logodds
gene
ENSG00000000003  -9.441596 -2.501993 -7.015090
ENSG00000000419  -9.906777 -2.812004 -6.547979
ENSG00000000457 -11.986061 -1.406747  0.337841
ENSG00000000460 -11.033319 -2.387189 -2.911735
ENSG00000001036 -11.051643 -2.351120 -2.723858
</pre>

<div class="org-src-container">
<pre class="src src-ipython">plt.clf()
plt.gcf().set_size_inches(9, 9)
<span class="org-variable-name">fig</span>, <span class="org-variable-name">ax</span> = plt.subplots(2, 2, gridspec_kw={<span class="org-string">'width_ratios'</span>: [0.5, 0.5], <span class="org-string">'height_ratios'</span>: [0.5, 0.5]})

ax[0][0].scatter(J[<span class="org-string">'log_mu'</span>], J[<span class="org-string">'log_phi'</span>], c=<span class="org-string">'k'</span>, s=2, alpha=0.25)
ax[0][0].set_xlabel(<span class="org-string">'$\ln(\mu)$'</span>)
ax[0][0].set_ylabel(<span class="org-string">'$\ln(\phi)$'</span>)

ax[1][0].scatter(J[<span class="org-string">'log_mu'</span>], J[<span class="org-string">'logodds'</span>], c=<span class="org-string">'k'</span>, s=2, alpha=0.25)
ax[1][0].set_xlabel(<span class="org-string">'$\ln(\mu)$'</span>)
ax[1][0].set_ylabel(<span class="org-string">'$\mathrm{logit}(\pi)$'</span>)

ax[0][1].scatter(J[<span class="org-string">'logodds'</span>], J[<span class="org-string">'log_phi'</span>], c=<span class="org-string">'k'</span>, s=2, alpha=0.25)
ax[0][1].set_xlabel(<span class="org-string">'$\mathrm{logit}(\pi)$'</span>)
ax[0][1].set_ylabel(<span class="org-string">'$\ln(\phi)$'</span>)

ax[1][1].axis(<span class="org-string">'off'</span>)

fig.tight_layout()
</pre>
</div>


<div class="figure">
<p><img src="figure/zinb.org/joint-distribution.png" alt="joint-distribution.png">
</p>
</div>
</div>
</div>

<div id="outline-container-org3d54e04" class="outline-2">
<h2 id="org3d54e04">Effect of confounding</h2>
<div class="outline-text-2" id="text-org3d54e04">
<p>
Estimate proportion of variance explained by confounders by estimating the
average reduction in heterogeneity (residual variance).
</p>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-variable-name">log_phi0</span> = pd.read_table(<span class="org-string">'/project2/mstephens/aksarkar/projects/singlecell-qtl/data/density-estimation/without-confounders/zi2-log-phi.txt.gz'</span>, index_col=0, sep=<span class="org-string">' '</span>)
<span class="org-variable-name">log_phi1</span> = pd.read_table(<span class="org-string">'/project2/mstephens/aksarkar/projects/singlecell-qtl/data/density-estimation/without-cell-cycle/zi2-log-phi.txt.gz'</span>, index_col=0, sep=<span class="org-string">' '</span>)
<span class="org-variable-name">log_phi2</span> = pd.read_table(<span class="org-string">'/project2/mstephens/aksarkar/projects/singlecell-qtl/data/density-estimation/with-cell-cycle-genes/zi2-log-phi.txt.gz'</span>, index_col=0, sep=<span class="org-string">' '</span>)
</pre>
</div>

<div class="org-src-container">
<pre class="src src-ipython">1 - (log_phi1 / log_phi0).mean().mean()
</pre>
</div>

<pre class="example">
0.4349981749938848

</pre>

<div class="org-src-container">
<pre class="src src-ipython">1 - (log_phi2 / log_phi1).mean().mean()
</pre>
</div>

<pre class="example">
0.0

</pre>

<p>
Estimate how much the mean changes due to confounding.
</p>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-variable-name">log_mu0</span> = pd.read_table(<span class="org-string">'/project2/mstephens/aksarkar/projects/singlecell-qtl/data/density-estimation/without-confounders/zi2-log-mu.txt.gz'</span>, index_col=0, sep=<span class="org-string">' '</span>)
<span class="org-variable-name">log_mu1</span> = pd.read_table(<span class="org-string">'/project2/mstephens/aksarkar/projects/singlecell-qtl/data/density-estimation/without-cell-cycle/zi2-log-mu.txt.gz'</span>, index_col=0, sep=<span class="org-string">' '</span>)
<span class="org-variable-name">log_mu2</span> = pd.read_table(<span class="org-string">'/project2/mstephens/aksarkar/projects/singlecell-qtl/data/density-estimation/with-cell-cycle-genes/zi2-log-mu.txt.gz'</span>, index_col=0, sep=<span class="org-string">' '</span>)
</pre>
</div>

<div class="org-src-container">
<pre class="src src-ipython">np.exp(log_mu0 - log_mu1).describe().loc[<span class="org-string">'mean'</span>].describe()
</pre>
</div>

<pre class="example">
count    54.000000
mean      1.334900
std       0.070728
min       1.195521
25%       1.280344
50%       1.343722
75%       1.370947
max       1.509227
Name: mean, dtype: float64
</pre>

<div class="org-src-container">
<pre class="src src-ipython">np.exp(log_mu1 - log_mu2).describe().loc[<span class="org-string">'mean'</span>].describe()
</pre>
</div>

<pre class="example">
count    54.0
mean      1.0
std       0.0
min       1.0
25%       1.0
50%       1.0
75%       1.0
max       1.0
Name: mean, dtype: float64
</pre>
</div>
</div>

<div id="outline-container-orgc80bb54" class="outline-2">
<h2 id="orgc80bb54">Mean-variance relationship by individual</h2>
<div class="outline-text-2" id="text-orgc80bb54">
<p>
Read the estimated parameters.
</p>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-keyword">with</span> sqlite3.connect(<span class="org-string">'/project2/mstephens/aksarkar/projects/singlecell-qtl/browser/browser.db'</span>) <span class="org-keyword">as</span> conn:
  <span class="org-variable-name">params</span> = pd.read_sql(<span class="org-string">'select * from params;'</span>, conn)
</pre>
</div>

<p>
Plot the mean-variance relationship for one individual.
</p>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-variable-name">subset</span> = params[params[<span class="org-string">'ind'</span>] == <span class="org-string">'NA18501'</span>]
<span class="org-variable-name">subset</span> = subset[subset[<span class="org-string">'mean'</span>] &gt; 0]
<span class="org-variable-name">grid</span> = np.linspace(subset[<span class="org-string">'mean'</span>].<span class="org-builtin">min</span>(), subset[<span class="org-string">'mean'</span>].<span class="org-builtin">max</span>(), 200)
plt.clf()
plt.gcf().set_size_inches(6, 6)
plt.scatter(subset[<span class="org-string">'mean'</span>], subset[<span class="org-string">'var'</span>] / subset[<span class="org-string">'mean'</span>], s=2, c=<span class="org-string">'k'</span>)
<span class="org-keyword">for</span> phi <span class="org-keyword">in</span> np.linspace(0, 1, 5):
  plt.plot(grid, (grid + np.square(grid) * phi) / (grid + 1e-8), label=<span class="org-string">'{:.2f}'</span>.<span class="org-builtin">format</span>(phi), c=colorcet.cm[<span class="org-string">'bkr'</span>](phi), ls=<span class="org-string">':'</span>, lw=1)
plt.legend(title=<span class="org-string">'Overdispersion'</span>)
plt.xlabel(<span class="org-string">'Estimated mean'</span>)
plt.ylabel(<span class="org-string">'Estimated Fano factor'</span>)
</pre>
</div>

<pre class="example">
Text(0,0.5,'Estimated Fano factor')

</pre>

<div class="figure">
<p><img src="figure/zinb.org/mean-var-rel.png" alt="mean-var-rel.png">
</p>
</div>
</div>
</div>

<div id="outline-container-org4f496c8" class="outline-2">
<h2 id="org4f496c8">Per-gene dispersion</h2>
<div class="outline-text-2" id="text-org4f496c8">
<p>
The index of dispersion for observed data \(r_{ijk}\) at gene \(k\) is:
</p>

<p>
\[ D_k = \frac{V[r_{ijk}]}{E[r_{ijk}]} \]
</p>

<p>
where expectations (variances) are taken over individuals \(i\) and cells
\(j\).
</p>

<p>
Let \(g_{ijk}\) denote the zero-inflated negative binomial density as defined
above. Then, we have:
</p>

<p>
\[ r_{ijk} \sim \sum_{ijk} \frac{1}{N} g_{ijk}(\cdot) \]
</p>

<p>
Fixing gene \(k\), the mixture density has expectation:
</p>

<p>
\[ \mu_k = \frac{1}{N} \sum E[r_{ijk}] \]
</p>

<p>
and variance (<a href="http://www.springer.com/us/book/9780387329093">Frhwirth-Schnatter 2006</a>):
</p>

<p>
\[ \sigma^2_k = \frac{1}{N} \sum (E[r_{ijk}] - \mu_k)^2 + V[r_{ijk}] \]
</p>

<p>
Fixing individual \(i\) and cell \(j\), we have:
</p>

<p>
\[ E[r_{ijk}] = R_{ij} \mu_{ik} \]
</p>

<p>
\[ V[r_{ijk}] = \left(R_{ij} \mu_{ik} + (R_{ij} \mu_{ik})^2 \phi_{ik}\right) \]
</p>

<p>
Here, we ignore the factor of \((1 - \pi_{ik})\) under the assumption that
excess zeros do not reflect biology.
</p>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-keyword">with</span> sqlite3.connect(<span class="org-string">'/project2/mstephens/aksarkar/projects/singlecell-qtl/browser/browser.db'</span>) <span class="org-keyword">as</span> conn:
  <span class="org-variable-name">params</span> = pd.read_sql(<span class="org-string">'select * from params;'</span>, conn)
<span class="org-variable-name">gene_params</span> = params.groupby(<span class="org-string">'gene'</span>).<span class="org-builtin">apply</span>(<span class="org-keyword">lambda</span> x: pd.Series([x[<span class="org-string">'mean'</span>].mean(), (np.square(x[<span class="org-string">'mean'</span>] - x[<span class="org-string">'mean'</span>].mean()) + x[<span class="org-string">'var'</span>]).mean()]))
</pre>
</div>

<p>
Plot the mean-variance relationship over all genes, estimated using all of
the data and accounting for the fact that data came from different
individuals.
</p>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-variable-name">subset</span> = gene_params[gene_params[0] &gt; 0]
<span class="org-variable-name">subset.columns</span> = [<span class="org-string">'mean'</span>, <span class="org-string">'var'</span>]
<span class="org-variable-name">lim</span> = [subset[<span class="org-string">'mean'</span>].<span class="org-builtin">min</span>(), subset[<span class="org-string">'mean'</span>].<span class="org-builtin">max</span>()]
<span class="org-variable-name">grid</span> = np.linspace(*lim, num=200)
plt.clf()
plt.gcf().set_size_inches(6, 6)
plt.scatter(subset[<span class="org-string">'mean'</span>], subset[<span class="org-string">'var'</span>] / subset[<span class="org-string">'mean'</span>], s=2, c=<span class="org-string">'k'</span>)
<span class="org-keyword">for</span> phi <span class="org-keyword">in</span> np.geomspace(.1, 2, 5):
  plt.plot(grid, (grid + np.square(grid) * phi) / (grid + 1e-8), label=<span class="org-string">'{:.2f}'</span>.<span class="org-builtin">format</span>(phi), c=colorcet.cm[<span class="org-string">'bkr'</span>](phi), ls=<span class="org-string">':'</span>, lw=1)
plt.legend(title=<span class="org-string">'Overdispersion'</span>)
plt.xlim(lim)
plt.ylim(lim)
plt.xlabel(<span class="org-string">'Estimated mean'</span>)
<span class="org-variable-name">_</span> = plt.ylabel(<span class="org-string">'Estimated Fano factor'</span>)
</pre>
</div>


<div class="figure">
<p><img src="figure/zinb.org/overall-mean-var-rel.png" alt="overall-mean-var-rel.png">
</p>
</div>

<p>
Bin the data and plot the distribution by bin.
</p>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-variable-name">n_bin</span> = 8
<span class="org-variable-name">subset</span>[<span class="org-string">'bin'</span>] = pd.cut(subset[<span class="org-string">'mean'</span>], n_bin)
<span class="org-variable-name">subset</span>[<span class="org-string">'fano'</span>] = subset[<span class="org-string">'var'</span>] / subset[<span class="org-string">'mean'</span>]
plt.clf()
plt.gcf().set_size_inches(5, 4)
<span class="org-keyword">for</span> i, (k, g) <span class="org-keyword">in</span> <span class="org-builtin">enumerate</span>(subset.groupby(<span class="org-string">'bin'</span>)):
  <span class="org-keyword">if</span> <span class="org-builtin">len</span>(g) &lt; 10:
    plt.scatter((i + .5) * np.ones(g[<span class="org-string">'fano'</span>].shape[0]), g[<span class="org-string">'fano'</span>], c=<span class="org-string">'k'</span>, s=4, label=k)
  <span class="org-keyword">else</span>:
    plt.boxplot(g[<span class="org-string">'fano'</span>], positions=[i + .5], sym=<span class="org-string">'.'</span>, widths=.25)
plt.xticks(np.arange(n_bin + 1), [<span class="org-string">'{:.2f}'</span>.<span class="org-builtin">format</span>(x) <span class="org-keyword">for</span> x <span class="org-keyword">in</span> np.linspace(0, 1, n_bin + 1)])
plt.xlim(0, n_bin)
plt.xlabel(<span class="org-string">'Quantile of estimated mean'</span>)
<span class="org-variable-name">_</span> = plt.ylabel(<span class="org-string">'Estimated Fano factor'</span>)
</pre>
</div>


<div class="figure">
<p><img src="figure/zinb.org/overall-mean-var-rel-by-quantile.png" alt="overall-mean-var-rel-by-quantile.png">
</p>
</div>
</div>
</div>
</div>
<div id="postamble" class="status">
<p class="author">Author: Abhishek Sarkar</p>
<p class="date">Created: 2018-05-08 Tue 14:21</p>
<p class="validation"><a href="http://validator.w3.org/check?uri=referer">Validate</a></p>
</div>
</body>
</html>
