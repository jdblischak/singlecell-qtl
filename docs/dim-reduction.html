<!DOCTYPE html>
<html lang="en">
<head>
<!-- 2018-01-29 Mon 19:12 -->
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1">
<title>Dimensionality reduction</title>
<meta name="generator" content="Org mode">
<meta name="author" content="Abhishek Sarkar">
<style type="text/css">
 <!--/*--><![CDATA[/*><!--*/
  .title  { text-align: center;
             margin-bottom: .2em; }
  .subtitle { text-align: center;
              font-size: medium;
              font-weight: bold;
              margin-top:0; }
  .todo   { font-family: monospace; color: red; }
  .done   { font-family: monospace; color: green; }
  .priority { font-family: monospace; color: orange; }
  .tag    { background-color: #eee; font-family: monospace;
            padding: 2px; font-size: 80%; font-weight: normal; }
  .timestamp { color: #bebebe; }
  .timestamp-kwd { color: #5f9ea0; }
  .org-right  { margin-left: auto; margin-right: 0px;  text-align: right; }
  .org-left   { margin-left: 0px;  margin-right: auto; text-align: left; }
  .org-center { margin-left: auto; margin-right: auto; text-align: center; }
  .underline { text-decoration: underline; }
  #postamble p, #preamble p { font-size: 90%; margin: .2em; }
  p.verse { margin-left: 3%; }
  pre {
    border: 1px solid #ccc;
    box-shadow: 3px 3px 3px #eee;
    padding: 8pt;
    font-family: monospace;
    overflow: auto;
    margin: 1.2em;
  }
  pre.src {
    position: relative;
    overflow: visible;
    padding-top: 1.2em;
  }
  pre.src:before {
    display: none;
    position: absolute;
    background-color: white;
    top: -10px;
    right: 10px;
    padding: 3px;
    border: 1px solid black;
  }
  pre.src:hover:before { display: inline;}
  /* Languages per Org manual */
  pre.src-asymptote:before { content: 'Asymptote'; }
  pre.src-awk:before { content: 'Awk'; }
  pre.src-C:before { content: 'C'; }
  /* pre.src-C++ doesn't work in CSS */
  pre.src-clojure:before { content: 'Clojure'; }
  pre.src-css:before { content: 'CSS'; }
  pre.src-D:before { content: 'D'; }
  pre.src-ditaa:before { content: 'ditaa'; }
  pre.src-dot:before { content: 'Graphviz'; }
  pre.src-calc:before { content: 'Emacs Calc'; }
  pre.src-emacs-lisp:before { content: 'Emacs Lisp'; }
  pre.src-fortran:before { content: 'Fortran'; }
  pre.src-gnuplot:before { content: 'gnuplot'; }
  pre.src-haskell:before { content: 'Haskell'; }
  pre.src-hledger:before { content: 'hledger'; }
  pre.src-java:before { content: 'Java'; }
  pre.src-js:before { content: 'Javascript'; }
  pre.src-latex:before { content: 'LaTeX'; }
  pre.src-ledger:before { content: 'Ledger'; }
  pre.src-lisp:before { content: 'Lisp'; }
  pre.src-lilypond:before { content: 'Lilypond'; }
  pre.src-lua:before { content: 'Lua'; }
  pre.src-matlab:before { content: 'MATLAB'; }
  pre.src-mscgen:before { content: 'Mscgen'; }
  pre.src-ocaml:before { content: 'Objective Caml'; }
  pre.src-octave:before { content: 'Octave'; }
  pre.src-org:before { content: 'Org mode'; }
  pre.src-oz:before { content: 'OZ'; }
  pre.src-plantuml:before { content: 'Plantuml'; }
  pre.src-processing:before { content: 'Processing.js'; }
  pre.src-python:before { content: 'Python'; }
  pre.src-R:before { content: 'R'; }
  pre.src-ruby:before { content: 'Ruby'; }
  pre.src-sass:before { content: 'Sass'; }
  pre.src-scheme:before { content: 'Scheme'; }
  pre.src-screen:before { content: 'Gnu Screen'; }
  pre.src-sed:before { content: 'Sed'; }
  pre.src-sh:before { content: 'shell'; }
  pre.src-sql:before { content: 'SQL'; }
  pre.src-sqlite:before { content: 'SQLite'; }
  /* additional languages in org.el's org-babel-load-languages alist */
  pre.src-forth:before { content: 'Forth'; }
  pre.src-io:before { content: 'IO'; }
  pre.src-J:before { content: 'J'; }
  pre.src-makefile:before { content: 'Makefile'; }
  pre.src-maxima:before { content: 'Maxima'; }
  pre.src-perl:before { content: 'Perl'; }
  pre.src-picolisp:before { content: 'Pico Lisp'; }
  pre.src-scala:before { content: 'Scala'; }
  pre.src-shell:before { content: 'Shell Script'; }
  pre.src-ebnf2ps:before { content: 'ebfn2ps'; }
  /* additional language identifiers per "defun org-babel-execute"
       in ob-*.el */
  pre.src-cpp:before  { content: 'C++'; }
  pre.src-abc:before  { content: 'ABC'; }
  pre.src-coq:before  { content: 'Coq'; }
  pre.src-groovy:before  { content: 'Groovy'; }
  /* additional language identifiers from org-babel-shell-names in
     ob-shell.el: ob-shell is the only babel language using a lambda to put
     the execution function name together. */
  pre.src-bash:before  { content: 'bash'; }
  pre.src-csh:before  { content: 'csh'; }
  pre.src-ash:before  { content: 'ash'; }
  pre.src-dash:before  { content: 'dash'; }
  pre.src-ksh:before  { content: 'ksh'; }
  pre.src-mksh:before  { content: 'mksh'; }
  pre.src-posh:before  { content: 'posh'; }
  /* Additional Emacs modes also supported by the LaTeX listings package */
  pre.src-ada:before { content: 'Ada'; }
  pre.src-asm:before { content: 'Assembler'; }
  pre.src-caml:before { content: 'Caml'; }
  pre.src-delphi:before { content: 'Delphi'; }
  pre.src-html:before { content: 'HTML'; }
  pre.src-idl:before { content: 'IDL'; }
  pre.src-mercury:before { content: 'Mercury'; }
  pre.src-metapost:before { content: 'MetaPost'; }
  pre.src-modula-2:before { content: 'Modula-2'; }
  pre.src-pascal:before { content: 'Pascal'; }
  pre.src-ps:before { content: 'PostScript'; }
  pre.src-prolog:before { content: 'Prolog'; }
  pre.src-simula:before { content: 'Simula'; }
  pre.src-tcl:before { content: 'tcl'; }
  pre.src-tex:before { content: 'TeX'; }
  pre.src-plain-tex:before { content: 'Plain TeX'; }
  pre.src-verilog:before { content: 'Verilog'; }
  pre.src-vhdl:before { content: 'VHDL'; }
  pre.src-xml:before { content: 'XML'; }
  pre.src-nxml:before { content: 'XML'; }
  /* add a generic configuration mode; LaTeX export needs an additional
     (add-to-list 'org-latex-listings-langs '(conf " ")) in .emacs */
  pre.src-conf:before { content: 'Configuration File'; }

  table { border-collapse:collapse; }
  caption.t-above { caption-side: top; }
  caption.t-bottom { caption-side: bottom; }
  td, th { vertical-align:top;  }
  th.org-right  { text-align: center;  }
  th.org-left   { text-align: center;   }
  th.org-center { text-align: center; }
  td.org-right  { text-align: right;  }
  td.org-left   { text-align: left;   }
  td.org-center { text-align: center; }
  dt { font-weight: bold; }
  .footpara { display: inline; }
  .footdef  { margin-bottom: 1em; }
  .figure { padding: 1em; }
  .figure p { text-align: center; }
  .inlinetask {
    padding: 10px;
    border: 2px solid gray;
    margin: 10px;
    background: #ffffcc;
  }
  #org-div-home-and-up
   { text-align: right; font-size: 70%; white-space: nowrap; }
  textarea { overflow-x: auto; }
  .linenr { font-size: smaller }
  .code-highlighted { background-color: #ffff00; }
  .org-info-js_info-navigation { border-style: none; }
  #org-info-js_console-label
    { font-size: 10px; font-weight: bold; white-space: nowrap; }
  .org-info-js_search-highlight
    { background-color: #ffff00; color: #000000; font-weight: bold; }
  .org-svg { width: 90%; }
  /*]]>*/-->
</style>
<link href="site_libs/bootstrap-3.3.5/css/cosmo.min.css" rel="stylesheet" />
<link rel="stylesheet" type="text/css" href="https://www.pirilampo.org/styles/readtheorg/css/htmlize.css"/>
<style type="text/css">body {width: 60em; margin:auto} pre.src {overflow:auto}</style>
<script type="text/javascript">
/*
@licstart  The following is the entire license notice for the
JavaScript code in this tag.

Copyright (C) 2012-2017 Free Software Foundation, Inc.

The JavaScript code in this tag is free software: you can
redistribute it and/or modify it under the terms of the GNU
General Public License (GNU GPL) as published by the Free Software
Foundation, either version 3 of the License, or (at your option)
any later version.  The code is distributed WITHOUT ANY WARRANTY;
without even the implied warranty of MERCHANTABILITY or FITNESS
FOR A PARTICULAR PURPOSE.  See the GNU GPL for more details.

As additional permission under GNU GPL version 3 section 7, you
may distribute non-source (e.g., minimized or compacted) forms of
that code without the copy of the GNU GPL normally required by
section 4, provided you include this license notice and a URL
through which recipients can access the Corresponding Source.


@licend  The above is the entire license notice
for the JavaScript code in this tag.
*/
<!--/*--><![CDATA[/*><!--*/
 function CodeHighlightOn(elem, id)
 {
   var target = document.getElementById(id);
   if(null != target) {
     elem.cacheClassElem = elem.className;
     elem.cacheClassTarget = target.className;
     target.className = "code-highlighted";
     elem.className   = "code-highlighted";
   }
 }
 function CodeHighlightOff(elem, id)
 {
   var target = document.getElementById(id);
   if(elem.cacheClassElem)
     elem.className = elem.cacheClassElem;
   if(elem.cacheClassTarget)
     target.className = elem.cacheClassTarget;
 }
/*]]>*///-->
</script>
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        displayAlign: "center",
        displayIndent: "0em",

        "HTML-CSS": { scale: 100,
                        linebreaks: { automatic: "false" },
                        webFont: "TeX"
                       },
        SVG: {scale: 100,
              linebreaks: { automatic: "false" },
              font: "TeX"},
        NativeMML: {scale: 100},
        TeX: { equationNumbers: {autoNumber: "AMS"},
               MultLineWidth: "85%",
               TagSide: "right",
               TagIndent: ".8em"
             }
});
</script>
<script type="text/javascript"
        src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS_HTML"></script>
</head>
<body>
<div id="content">
<h1 class="title">Dimensionality reduction</h1>

<div id="outline-container-orgd77f498" class="outline-2">
<h2 id="orgd77f498">Introduction</h2>
<div class="outline-text-2" id="text-orgd77f498">
<p>
The fundamental inference task is to infer \(p(z_i \mid x_i)\), where \(x_i\)
is the \(p\)-dimensional observation (one sample), \(z_i\) is a
\(k\)-dimensional latent variable, and \(k \ll n\).
</p>

<p>
Why do we want to do this? 
</p>
<ul class="org-ul">
<li>determine how much variation in the data is explained by known technical
factors</li>
<li>remove that variation before trying to explain the data using biological
covariates</li>
</ul>

<p>
The typical procedure to apply PCA to single cell RNA-Seq data is:
</p>

<ol class="org-ol">
<li>Normalize counts</li>
<li>Select a subset of genes, e.g. those detected in some minimum proportion
of cells, with high mean/variability of expression, etc.</li>
<li>Compute singular vectors (principal components) of the count matrix</li>
<li>Take loadings on the top \(k\) singular vectors as the latent \(z_i\)</li>
<li>Correlate the loadings with the known covariates.</li>
</ol>

<p>
Assuming this is the right way to quantify how much variation in the data is
explained by covariates, there are several problems with this
procedure. Prior work showed that (probabilistic) PCA fails to accurately
infer \(z_i\) under a variety of scenarios (<a href="https://dx.doi.org/10.1186/s13059-015-0805-z">Pierson et al 2015</a>):
</p>

<ul class="org-ul">
<li>high missingness</li>
<li>stringent gene selection</li>
</ul>

<p>
Several methods have been proposed to improve inference of latent gene
expression:
</p>

<ul class="org-ul">
<li>ZIFA (<a href="https://dx.doi.org/10.1186/s13059-015-0805-z">Pierson et al 2015</a>)</li>
<li>ZINB-WAVE (<a href="https://www.nature.com/articles/s41467-017-02554-5">Risso et al 2018</a>)</li>
<li>scVI (<a href="https://arxiv.org/abs/1709.02082">Lopez et al 2017</a>)</li>
<li>countae (Eraslen et al 2017)</li>
</ul>

<p>
The main disadvantage of the first two methods is high computational cost and
poor scaling with data size. The main disadvantage of the last two is they
rely on neural networks making interpretation more difficult.
</p>

<p>
It is also worth asking whether this is actually the right way to quantify
how much variation in the data is explained by covariates. A different, but
still natural way to phrase the question is to ask whether the expression
values can be accurately predicted using known covariates. This procedure
naturally suggests a confounder correction method: regress out known
covariates from the expression values.
</p>

<p>
This idea is related to half-sibling regression (<a href="http://www.pnas.org/cgi/doi/10.1073/pnas.1511656113">Sch√∂lkopf et al 2016</a>): we
can only accurately predict gene expression using genes on other chromosomes
if there is true <i>trans</i>-regulation or systematic confounding. Again, the
natural solution is to regress out the rest of the genome from the gene
expression values.
</p>

<p>
Importantly, these analyses are not directly usable for confounder correction
for QTL mapping. Instead, we first need to <a href="zinb.html">learn the underlying distributions
of the data</a> and then perform dimensionality reduction on those parameters.
</p>

<p>
Here, we perform the following analyses:
</p>

<ol class="org-ol">
<li><a href="#orgff849ef">We perform PCA on the post-QC data</a> and show that most variation is
still explained by variation in sequencing metrics</li>
<li><a href="#org266705d">We provide an accelerated implementation of zero-inflated factor analysis</a></li>
<li><a href="#org9f49d24">We fit a variational autoencoder to the data</a></li>
<li><a href="#org9923084">We predict mean expression and dropout probability using covariates only</a></li>
</ol>
</div>
</div>

<div id="outline-container-org8597b6f" class="outline-2">
<h2 id="org8597b6f">Read the data</h2>
<div class="outline-text-2" id="text-org8597b6f">
<p>
Read the full data matrix and apply the QC filters.
</p>

<div class="org-src-container">
<pre class="src src-ipython" id="org6b29df0"><span class="org-variable-name">umi</span> = pd.read_table(<span class="org-string">'/home/aksarkar/projects/singlecell-qtl/data/scqtl-counts.txt.gz'</span>, index_col=0)
<span class="org-variable-name">annotations</span> = pd.read_table(<span class="org-string">'/home/aksarkar/projects/singlecell-qtl/data/scqtl-annotation.txt'</span>)
<span class="org-variable-name">keep_samples</span> = pd.read_table(<span class="org-string">'/home/aksarkar/projects/singlecell-qtl/data/quality-single-cells.txt'</span>, index_col=0, header=<span class="org-constant">None</span>)
<span class="org-variable-name">keep_genes</span> = pd.read_table(<span class="org-string">'/home/aksarkar/projects/singlecell-qtl/data/genes-pass-filter.txt'</span>, index_col=0, header=<span class="org-constant">None</span>)
<span class="org-variable-name">umi</span> = umi.loc[keep_genes.values.ravel(),keep_samples.values.ravel()]
<span class="org-variable-name">annotations</span> = annotations.loc[keep_samples.values.ravel()]
</pre>
</div>

<div class="org-src-container">
<pre class="src src-ipython">umi.shape
</pre>
</div>

<pre class="example">
(10886, 3910)

</pre>
</div>
</div>

<div id="outline-container-orgff849ef" class="outline-2">
<h2 id="orgff849ef">Principal components analysis</h2>
<div class="outline-text-2" id="text-orgff849ef">
<p>
Use PPCA (<a href="http://www.miketipping.com/papers/met-mppca.pdf">Tipping et al 1999</a>) to incorporate gene-specific mean
expression. Use the <code>edgeR</code> psuedocount.
</p>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-variable-name">libsize</span> = umi.<span class="org-builtin">sum</span>(axis=0)
<span class="org-variable-name">psuedocount</span> = .5 * libsize / libsize.mean()
<span class="org-variable-name">log_cpm</span> = (np.log(umi + psuedocount) - np.log(libsize + 2 * psuedocount) + 6 * np.log(10)) / np.log(2)
<span class="org-variable-name">ppca</span> = skd.PCA(n_components=10)
<span class="org-variable-name">loadings</span> = ppca.fit_transform(log_cpm.values.T)
</pre>
</div>

<div class="org-src-container">
<pre class="src src-ipython">plt.clf()
<span class="org-variable-name">fig</span>, <span class="org-variable-name">ax</span> = plt.subplots(2, 2)
fig.set_size_inches(12, 12)
<span class="org-keyword">for</span> i <span class="org-keyword">in</span> <span class="org-builtin">range</span>(2):
  <span class="org-keyword">for</span> j <span class="org-keyword">in</span> <span class="org-builtin">range</span>(i, 2):
    ax[i][j].scatter(loadings[:,i], loadings[:,j + 1])
    ax[i][j].set_xlabel(<span class="org-string">'PC{}'</span>.<span class="org-builtin">format</span>(j + 2))
    ax[i][j].set_ylabel(<span class="org-string">'PC{}'</span>.<span class="org-builtin">format</span>(i + 1))
</pre>
</div>


<div class="figure">
<p><img src="figure/dim-reduction.org/pca.png" alt="pca.png">
</p>
</div>

<p>
Correlate PCs with known continuous covariates.
</p>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-keyword">def</span> <span class="org-function-name">extract_covars</span>(annotations):
  <span class="org-keyword">return</span> pd.Series({
    <span class="org-string">'batch'</span>: <span class="org-builtin">int</span>(annotations[<span class="org-string">'batch'</span>][1:]),
    <span class="org-comment-delimiter"># </span><span class="org-comment">Recode experiment YYYYMMDD so it is strictly increasing with time</span>
    <span class="org-string">'experiment'</span>: 10000 * (annotations[<span class="org-string">'experiment'</span>] % 10000) + annotations[<span class="org-string">'experiment'</span>] // 10000,
    <span class="org-string">'index'</span>: annotations[<span class="org-string">'index'</span>],
    <span class="org-string">'concentration'</span>: annotations[<span class="org-string">'concentration'</span>],
    <span class="org-string">'reads'</span>: annotations[<span class="org-string">'raw'</span>],
    <span class="org-string">'mols'</span>: annotations[<span class="org-string">'molecules'</span>],
    <span class="org-string">'mapped_prop_hs'</span>: annotations[<span class="org-string">'mapped'</span>] / annotations[<span class="org-string">'raw'</span>],
    <span class="org-string">'mapped_prop_dm'</span>: annotations[<span class="org-string">'reads_dm'</span>] / annotations[<span class="org-string">'raw'</span>],
    <span class="org-string">'mapped_prop_ce'</span>: annotations[<span class="org-string">'reads_ce'</span>] / annotations[<span class="org-string">'raw'</span>],
    <span class="org-string">'detect_hs'</span>: annotations[<span class="org-string">'detect_hs'</span>],
    <span class="org-string">'chipmix'</span>: annotations[<span class="org-string">'chipmix'</span>],
    <span class="org-string">'freemix'</span>: annotations[<span class="org-string">'freemix'</span>],
  })
</pre>
</div>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-variable-name">cont_covars</span> = annotations.<span class="org-builtin">apply</span>(extract_covars, axis=1)
</pre>
</div>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-keyword">def</span> <span class="org-function-name">correlation</span>(pcs, cont_covars):
  <span class="org-doc">"""Return squared correlation between principal components and covariates</span>

<span class="org-doc">  pcs - DataFrame (n x k)</span>
<span class="org-doc">  cont_covars - DataFrame (n x q)</span>

<span class="org-doc">  """</span>
  <span class="org-variable-name">result</span> = []
  <span class="org-keyword">for</span> i <span class="org-keyword">in</span> pcs:
    <span class="org-keyword">for</span> j <span class="org-keyword">in</span> cont_covars:
      <span class="org-variable-name">keep</span> = np.isfinite(cont_covars[j].values)
      result.append([i, j, np.square(st.pearsonr(pcs[i][keep], cont_covars[j][keep]))[0]])
  <span class="org-keyword">return</span> pd.DataFrame(result, columns=[<span class="org-string">'pc'</span>, <span class="org-string">'covar'</span>, <span class="org-string">'corr'</span>])
</pre>
</div>

<p>
Correlating with individual is non-obvious because it is a categorical
variable, and simply recoding it as integer is sensitive to
ordering. Instead, regress the loading of each cell on each principal
component \(l_{ij}\) against indicator variables for each individual
\(X_{ik}\).
</p>

<p>
\[ l_{ij} = \sum_j X_{ik} \beta_{jk} + \mu + \epsilon \]
</p>

<p>
From the regression fit, we can compute the coefficient of determination
\(R^2\) for each PC \(j\):
</p>

<p>
\[ 1 - \frac{l_j - X \hat{\beta}_j}{l_j - \bar{l_j}} \]
</p>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-keyword">def</span> <span class="org-function-name">categorical_r2</span>(loadings, annotations, key):
  <span class="org-variable-name">categories</span> = <span class="org-builtin">sorted</span>(annotations[key].unique())
  <span class="org-variable-name">onehot</span> = np.zeros((annotations.shape[0], <span class="org-builtin">len</span>(categories)), dtype=np.float32)
  onehot[np.arange(onehot.shape[0]), annotations[key].<span class="org-builtin">apply</span>(<span class="org-keyword">lambda</span> x: categories.index(x))] = 1
  <span class="org-variable-name">m</span> = sklm.LinearRegression(fit_intercept=<span class="org-constant">True</span>, copy_X=<span class="org-constant">True</span>).fit(onehot, loadings)
  <span class="org-keyword">return</span> pd.DataFrame({
      <span class="org-string">'pc'</span>: np.arange(10),
      <span class="org-string">'covar'</span>: key,
      <span class="org-string">'corr'</span>: 1 - np.square(loadings - m.predict(onehot)).<span class="org-builtin">sum</span>(axis=0) / np.square(loadings - loadings.mean(axis=0)).<span class="org-builtin">sum</span>(axis=0)})
</pre>
</div>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-variable-name">cat_covars</span> = annotations[[<span class="org-string">'chip_id'</span>, <span class="org-string">'well'</span>]]

<span class="org-variable-name">corr</span> = pd.concat(
  [correlation(pd.DataFrame(loadings), cont_covars)] +
  [categorical_r2(loadings, annotations, k) <span class="org-keyword">for</span> k <span class="org-keyword">in</span> cat_covars])
<span class="org-variable-name">corr</span> = corr.pivot(index=<span class="org-string">'covar'</span>, columns=<span class="org-string">'pc'</span>)
</pre>
</div>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-keyword">def</span> <span class="org-function-name">plot_pca_covar_corr</span>(pca, corr):
  plt.clf()
  <span class="org-variable-name">fig</span>, <span class="org-variable-name">ax</span> = plt.subplots(2, 1, gridspec_kw={<span class="org-string">'height_ratios'</span>: [.25, .75]}, sharex=<span class="org-constant">True</span>)
  fig.set_size_inches(8, 12)
  ax[0].bar(np.arange(<span class="org-builtin">len</span>(pca.components_)), pca.explained_variance_ratio_)
  ax[0].set_xticks(np.arange(<span class="org-builtin">len</span>(pca.components_)))
  ax[0].set_xticklabels([<span class="org-builtin">str</span>(x) <span class="org-keyword">for</span> x <span class="org-keyword">in</span> np.arange(1, <span class="org-builtin">len</span>(pca.components_) + 1)])
  ax[0].set_xlabel(<span class="org-string">'Principal component'</span>)
  ax[0].set_ylabel(<span class="org-string">'PVE'</span>)

  <span class="org-variable-name">im</span> = ax[1].imshow(corr.values, cmap=colorcet.cm[<span class="org-string">'fire'</span>], vmin=0, vmax=1, aspect=<span class="org-string">'auto'</span>)
  <span class="org-variable-name">cb</span> = plt.colorbar(im, ax=ax[1], orientation=<span class="org-string">'horizontal'</span>)
  cb.set_label(<span class="org-string">'Squared correlation'</span>)
  ax[1].set_xlabel(<span class="org-string">'Principal component'</span>)
  ax[1].set_yticks(np.arange(corr.shape[0]))
  ax[1].set_yticklabels(corr.index)
  ax[1].set_ylabel(<span class="org-string">'Covariate'</span>)
</pre>
</div>

<div class="org-src-container">
<pre class="src src-ipython">plot_pca_covar_corr(ppca, corr)
</pre>
</div>


<div class="figure">
<p><img src="figure/dim-reduction.org/pca-vs-covars.png" alt="pca-vs-covars.png">
</p>
</div>

<p>
The top 10 PCs define a low-rank approximation to the original data, so we
should ask how good the approximation was, by comparing the distribution of
the original data to the distribution of the reconstructed data.
</p>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-variable-name">reconstructed</span> = ppca.inverse_transform(loadings)
</pre>
</div>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-keyword">def</span> <span class="org-function-name">plot_reconstruction</span>(obs, approx):
  plt.clf()
  plt.hist(obs, bins=50, density=<span class="org-constant">True</span>, histtype=<span class="org-string">'step'</span>, color=<span class="org-string">'k'</span>, label=<span class="org-string">'Observed'</span>)
  plt.hist(approx, bins=50, density=<span class="org-constant">True</span>, histtype=<span class="org-string">'step'</span>, color=<span class="org-string">'r'</span>, label=<span class="org-string">'Reconstructed'</span>)
  plt.legend()
  plt.xlabel(<span class="org-string">'$\log_2(\mathrm{CPM} + 1)$'</span>)
  plt.ylabel(<span class="org-string">'Empirical density'</span>)
</pre>
</div>

<p>
For genes with high proportion of zero counts, the low-rank approximation is
mainly capturing the mean of the data, which is maybe more indicative of the
zero proportion in the data rather than the actual mean of the data.
</p>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-variable-name">num_zero</span> = np.isclose(umi, 0).<span class="org-builtin">sum</span>(axis=1)
<span class="org-variable-name">max_zero</span> = num_zero.argmax()
plot_reconstruction(log_cpm.iloc[max_zero], reconstructed[:,max_zero])
</pre>
</div>


<div class="figure">
<p><img src="figure/dim-reduction.org/reconstruction-error-max-zero.png" alt="reconstruction-error-max-zero.png">
</p>
</div>

<p>
This is true even for genes with the lowest proportion of zero counts.
</p>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-variable-name">min_zero</span> = num_zero.argmin()
plot_reconstruction(log_cpm.iloc[min_zero], reconstructed[:,min_zero])
</pre>
</div>


<div class="figure">
<p><img src="figure/dim-reduction.org/reconstruction-error-min-zero.png" alt="reconstruction-error-min-zero.png">
</p>
</div>

<p>
This might still be OK, if the reconstructed gene expression values are
predictive of the original gene expression values.
</p>

<div class="org-src-container">
<pre class="src src-ipython"><span class="org-variable-name">pred_score</span> = [sklm.LinearRegression(fit_intercept=<span class="org-constant">True</span>).fit(x.values.reshape(-1, 1), y).score(x.values.reshape(-1, 1), y)
              <span class="org-keyword">for</span> (_, x), (_, y)
              <span class="org-keyword">in</span> <span class="org-builtin">zip</span>(log_cpm.iteritems(),
                     pd.DataFrame(reconstructed.T).iteritems())]
</pre>
</div>

<div class="org-src-container">
<pre class="src src-ipython">plt.clf()
plt.hist(pred_score, bins=50)
plt.xlabel(<span class="org-string">'Prediction $R^2$'</span>)
plt.ylabel(<span class="org-string">'Number of genes'</span>)
plt.title(<span class="org-string">'Correlation between PCA and original data'</span>)
</pre>
</div>

<pre class="example">
Text(0.5,1,'Correlation between PCA and original data')

</pre>

<div class="figure">
<p><img src="figure/dim-reduction.org/reconstruction-pred-score.png" alt="reconstruction-pred-score.png">
</p>
</div>

<p>
The distribution of squared correlations suggest that the low rank
approximation is better for some genes than others, i.e. that there could be
gene-specific or gene module-specific effects. These are unlikely to be
captured by PCA or factor analysis.
</p>
</div>
</div>

<div id="outline-container-org266705d" class="outline-2">
<h2 id="org266705d">Zero-inflated factor analysis</h2>
<div class="outline-text-2" id="text-org266705d">
<p>
Fit ZIFA (<a href="https://dx.doi.org/10.1186/s13059-015-0805-z">Pierson et al 2015</a>).
</p>

<div class="org-src-container">
<pre class="src src-ipython">&lt;&lt;dim-reduction-imports&gt;&gt;
&lt;&lt;read-data&gt;&gt;
&lt;&lt;log_cpm&gt;&gt;
<span class="org-variable-name">latent</span>, <span class="org-variable-name">params</span> = zifa.fitModel(Y=log_cpm.values.T, K=10, p0_thresh=.7)
np.savetxt(<span class="org-string">'latent.txt.gz'</span>, latent)
</pre>
</div>

<div class="org-src-container">
<pre class="src src-sh">sbatch --partition=broadwl --time=400 --mem=16G --out=zifa.out --err zifa.err
<span class="org-comment-delimiter">#</span><span class="org-comment">!/bin/bash</span>
<span class="org-builtin">source</span> activate scqtl
python zifa.py
</pre>
</div>
</div>
</div>

<div id="outline-container-org9f49d24" class="outline-2">
<h2 id="org9f49d24">Variational autoencoder</h2>
<div class="outline-text-2" id="text-org9f49d24">
<p>
PCA could be sensitive to normalization and zeros, so fit a <a href="https://github.com/gokceneraslan/countae">deep generative
model directly on counts</a> instead. This method scales better than fitting ZINB
directly via maximum likelihood because the number of parameters doesn't grow
with the size of the data.
</p>

<p>
\[ p(x_i \mid z_i) = \mathrm{ZINB}(\pi(z_i), \lambda(z_i), \phi(z_i)) \]
</p>

<p>
\[ p(z_i) = N(0, I) \]
</p>

<p>
\[ q(z_i \mid x_i) = N(\mu(x), \sigma^2(x)) \]
</p>

<div class="org-src-container">
<pre class="src src-sh">sbatch --partition=gpu2 --gres=gpu:1 --mem=16G --out vae.out
<span class="org-comment-delimiter">#</span><span class="org-comment">!/bin/bash</span>
<span class="org-builtin">source</span> activate scqtl
autoencoder /home/aksarkar/projects/singlecell-qtl/data/scqtl-counts.txt.gz .
</pre>
</div>
</div>
</div>
<div id="outline-container-org9923084" class="outline-2">
<h2 id="org9923084">Predict the data using covariates</h2>
</div>
</div>
<div id="postamble" class="status">
<p class="author">Author: Abhishek Sarkar</p>
<p class="date">Created: 2018-01-29 Mon 19:12</p>
<p class="validation"><a href="http://validator.w3.org/check?uri=referer">Validate</a></p>
</div>
</body>
</html>
