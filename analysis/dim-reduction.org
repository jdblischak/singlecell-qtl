#+TITLE: Dimensionality reduction
#+OPTIONS: toc:nil
#+SETUPFILE: setup.org

* Introduction

  The fundamental inference task is to infer \(p(z_i \mid x_i)\), where \(x_i\)
  is a \(p\)-dimensional observation, \(z_i\) is a \(k\)-dimensional latent
  variable, and \(k \ll n\).

  Why do we want to do this?

  - determine how much variation in the data is explained by known technical
    factors
  - decide whether, and how to remove that variation before trying to explain
    the data using biological covariates

  Importantly, these analyses are not directly usable for confounder correction
  for QTL mapping. Instead, we first need to [[file:zinb.org][learn the underlying distributions
  of the data]] and then perform dimensionality reduction on those
  parameters. However, it will be important to consider what data went into
  learning those distributions, and how to incorporate known and inferred
  confounders into that estimation procedure.

  Here, we perform the following analyses:

  1. [[*Principal components analysis][We perform PCA on the post-QC data]] and show that most variation is
     explained by gene detection rate
  2. [[*Zero-inflated factor analysis][We fit zero-inflated factor analysis]] and show that after explicitly
     modeling dropout, we still recover a factor correlated with gene detection
     rate
  3. [[*Effect of normalization on PCA][We show in simulation]] that PC1 is correlated with library size even in the
     absence of dropout
  4. [[*Effect of dropout on gene expression][We confirm in the real data]] that the entire distribution of non-zero gene
     expression is correlated with gene detection rate
  5. [[*PCA on bicentered data][We show that adjusting for both sample- and gene-specific means]] eliminates
     the first technical PC
  6. [[*Kernel PCA][We show that applying kernel PCA]] eliminates the second technical PC
  7. [[*PCA on gene expression residuals][We show that regressing out the percentiles of gene expression]] eliminates
     both technical PCs

* Setup                                                            :noexport:

  #+BEGIN_SRC emacs-lisp
    (org-babel-lob-ingest "/home/aksarkar/.emacs.d/org-templates/library.org")
  #+END_SRC

  #+RESULTS:
  : 1

  #+CALL: ipython3(memory="16G",venv="scqtl") :dir /scratch/midway2/aksarkar/singlecell

  #+RESULTS:
  : Submitted batch job 43009166

  #+BEGIN_SRC ipython
    %matplotlib inline
  #+END_SRC

  #+RESULTS:
  :RESULTS:
  # Out[17]:
  :END:

  #+NAME: dim-reduction-imports
  #+BEGIN_SRC ipython
    import colorcet
    import functools
    import matplotlib.pyplot as plt
    import numpy as np
    import pandas as pd
    import scipy.stats as st
    import sklearn.decomposition as skd
    import sklearn.linear_model as sklm
    import ZIFA.block_ZIFA as zifa
  #+END_SRC

  #+RESULTS: dim-reduction-imports
  :RESULTS:
  # Out[2]:
  :END:

* Read the data

  Read the full data matrix and apply the QC filters.

  #+NAME: read-data-qc
  #+BEGIN_SRC ipython
    umi = pd.read_table('/project2/mstephens/aksarkar/projects/singlecell-qtl/data/scqtl-counts.txt.gz', index_col=0)
    annotations = pd.read_table('/project2/mstephens/aksarkar/projects/singlecell-qtl/data/scqtl-annotation.txt')
    keep_samples = pd.read_table('/project2/mstephens/aksarkar/projects/singlecell-qtl/data/quality-single-cells.txt', index_col=0, header=None)
    keep_genes = pd.read_table('/project2/mstephens/aksarkar/projects/singlecell-qtl/data/genes-pass-filter.txt', index_col=0, header=None)
    umi = umi.loc[keep_genes.values.ravel(),keep_samples.values.ravel()]
    annotations = annotations.loc[keep_samples.values.ravel()]
  #+END_SRC

  #+RESULTS: read-data-qc
  :RESULTS:
  # Out[3]:
  :END:

  #+BEGIN_SRC ipython
    umi.shape
  #+END_SRC

  #+RESULTS:
  :RESULTS:
  # Out[4]:
  : (10978, 4865)
  :END:

  The expected ERCC spike-in molecule counts were previously tabulated for
  1:50\(\times\) dilution.

  #+BEGIN_SRC ipython
    ercc_mean_mols_50x = pd.read_table('https://raw.githubusercontent.com/jdblischak/singleCellSeq/master/data/expected-ercc-molecules.txt')
  #+END_SRC

  #+RESULTS:
  :RESULTS:
  # Out[5]:
  :END:

  #+BEGIN_SRC ipython
    def efficiency(sample, ercc_mean_mols_50x):
      res = sample['mol_ercc'] / ercc_mean_mols_50x['ercc_molecules_well'].sum()
      if sample['ERCC'] == '50x dilution':
        return res
      elif sample['ERCC'] == '100x dilution':
        return .5 * res
      else:
        return np.nan
  #+END_SRC

  #+RESULTS:
  :RESULTS:
  # Out[6]:
  :END:

* Principal components analysis

  Use PPCA ([[http://www.miketipping.com/papers/met-mppca.pdf][Tipping et al 1999]]) to incorporate gene-specific mean
  expression. Use the ~edgeR~ pseudocount.

  #+NAME: normalize
  #+BEGIN_SRC ipython
    libsize = umi.sum(axis=0)
    pseudocount = .5 * libsize / libsize.mean()
    log_cpm = (np.log(umi + pseudocount) - np.log(libsize + 2 * pseudocount) + 6 * np.log(10)) / np.log(2)
  #+END_SRC

  #+RESULTS: normalize
  :RESULTS:
  # Out[6]:
  :END:

  #+BEGIN_SRC ipython
    ppca = skd.PCA(n_components=10)
    loadings = ppca.fit_transform(log_cpm.values.T)
  #+END_SRC

  #+RESULTS:
  :RESULTS:
  # Out[8]:
  :END:

  #+BEGIN_SRC ipython :ipyfile figure/dim-reduction.org/pca.png
    plt.clf()
    fig, ax = plt.subplots(2, 2)
    fig.set_size_inches(12, 12)
    for i in range(2):
      for j in range(i, 2):
        ax[i][j].scatter(loadings[:,i], loadings[:,j + 1])
        ax[i][j].set_xlabel('PC{}'.format(j + 2))
        ax[i][j].set_ylabel('PC{}'.format(i + 1))
  #+END_SRC

  #+RESULTS:
  :RESULTS:
  # Out[9]:
  [[file:figure/dim-reduction.org/pca.png]]
  :END:

  Correlate PCs with known continuous covariates by computing squared Pearson
  correlation.

  Correlating PCs with individual (or other discrete covariates) is non-obvious
  because it is a categorical variable, and simply recoding it as integer is
  sensitive to ordering. Instead, regress the loading of each cell on each
  principal component \(l_{ij}\) against indicator variables for each
  individual \(X_{ik}\).

  \[ l_{ij} = \sum_j X_{ik} \beta_{jk} + \mu + \epsilon \]

  From the regression fit, we can compute the coefficient of determination
  \(R^2\) for each PC \(j\):

  \[ 1 - \frac{l_j - X \hat{\beta}_j}{l_j - \bar{l_j}} \]

  #+NAME: corr-def
  #+BEGIN_SRC ipython
    def extract_covars(annotations):
      return pd.Series({
        'batch': int(annotations['batch'][1:]),
        # Recode experiment YYYYMMDD so it is strictly increasing with time
        'experiment': 10000 * (annotations['experiment'] % 10000) + annotations['experiment'] // 10000,
        'index': annotations['index'],
        'concentration': annotations['concentration'],
        'reads_with_umi': annotations['umi'],
        'mols': annotations['molecules'],
        'efficiency': efficiency(annotations, ercc_mean_mols_50x=ercc_mean_mols_50x),
        'mapped_prop_hs': annotations['reads_hs'] / annotations['mapped'],
        'mapped_prop_dm': annotations['reads_dm'] / annotations['mapped'],
        'mapped_prop_ce': annotations['reads_ce'] / annotations['mapped'],
        'detect_hs': annotations['detect_hs'],
        'chipmix': annotations['chipmix'],
        'freemix': annotations['freemix'],
      })

    def correlation(pcs, cont_covars):
      """Return squared correlation between principal components and covariates

      pcs - DataFrame (n x k)
      cont_covars - DataFrame (n x q)

      """
      result = []
      for i in pcs:
        for j in cont_covars:
          keep = np.isfinite(cont_covars[j].values)
          result.append([i, j, np.square(st.pearsonr(pcs[i][keep], cont_covars[j][keep]))[0]])
      return pd.DataFrame(result, columns=['pc', 'covar', 'corr'])

    def categorical_r2(loadings, annotations, key):
      categories = sorted(annotations[key].unique())
      onehot = np.zeros((annotations.shape[0], len(categories)), dtype=np.float32)
      onehot[np.arange(onehot.shape[0]), annotations[key].apply(lambda x: categories.index(x))] = 1
      m = sklm.LinearRegression(fit_intercept=True, copy_X=True).fit(onehot, loadings)
      return pd.DataFrame({
          'pc': np.arange(10),
          'covar': key,
          'corr': 1 - np.square(loadings - m.predict(onehot)).sum(axis=0) / np.square(loadings - loadings.mean(axis=0)).sum(axis=0)})
  #+END_SRC

  #+RESULTS:
  :RESULTS:
  # Out[11]:
  :END:

  #+NAME: extract-covars
  #+BEGIN_SRC ipython
    cont_covars = annotations.apply(extract_covars, axis=1)
    cat_covars = annotations[['chip_id', 'well']]
  #+END_SRC

  #+RESULTS:
  :RESULTS:
  # Out[15]:
  :END:

  #+BEGIN_SRC ipython
    corr = pd.concat(
      [correlation(pd.DataFrame(loadings), cont_covars)] +
      [categorical_r2(loadings, annotations, k) for k in cat_covars])
    corr = corr.pivot(index='covar', columns='pc')
  #+END_SRC

  #+RESULTS:
  :RESULTS:
  # Out[14]:
  :END:

  #+NAME: plot-covars-def
  #+BEGIN_SRC ipython
    def plot_pca_covar_corr(pca, corr):
      plt.clf()
      fig, ax = plt.subplots(2, 1, gridspec_kw={'height_ratios': [.25, .75]}, sharex=True)
      fig.set_size_inches(8, 12)
      ax[0].bar(np.arange(len(pca.components_)), pca.explained_variance_ratio_)
      ax[0].set_xticks(np.arange(len(pca.components_)))
      ax[0].set_xticklabels([str(x) for x in np.arange(1, len(pca.components_) + 1)])
      ax[0].set_xlabel('Principal component')
      ax[0].set_ylabel('PVE')

      im = ax[1].imshow(corr.values, cmap=colorcet.cm['fire'], vmin=0, vmax=1, aspect='auto')
      cb = plt.colorbar(im, ax=ax[1], orientation='horizontal')
      cb.set_label('Squared correlation')
      ax[1].set_xlabel('Principal component')
      ax[1].set_yticks(np.arange(corr.shape[0]))
      ax[1].set_yticklabels(corr.index)
      ax[1].set_ylabel('Covariate')
  #+END_SRC

  #+RESULTS:
  :RESULTS:
  # Out[28]:
  :END:

  #+BEGIN_SRC ipython :ipyfile figure/dim-reduction.org/pca-vs-covars.png
    plot_pca_covar_corr(ppca, corr)
  #+END_SRC

  #+RESULTS:
  :RESULTS:
  # Out[16]:
  [[file:figure/dim-reduction.org/pca-vs-covars.png]]
  :END:

  The top 10 PCs define a low-rank approximation to the original data, so we
  should ask how good the approximation was, by comparing the distribution of
  the original data to the distribution of the reconstructed data.

  #+BEGIN_SRC ipython
    reconstructed = ppca.inverse_transform(loadings)
  #+END_SRC

  #+RESULTS:
  :RESULTS:
  # Out[17]:
  :END:

  #+BEGIN_SRC ipython
    def plot_reconstruction(obs, approx):
      plt.clf()
      plt.hist(obs, bins=50, density=True, histtype='step', color='k', label='Observed')
      plt.hist(approx, bins=50, density=True, histtype='step', color='r', label='Reconstructed')
      plt.legend()
      plt.xlabel('$\log_2(\mathrm{CPM} + 1)$')
      plt.ylabel('Empirical density')
  #+END_SRC

  #+RESULTS:
  :RESULTS:
  # Out[18]:
  :END:

  For genes with high proportion of zero counts, the low-rank approximation is
  mainly capturing the mean of the data, which is maybe more indicative of the
  zero proportion in the data rather than the actual mean of the data.

  #+BEGIN_SRC ipython :ipyfile figure/dim-reduction.org/reconstruction-error-max-zero.png
    num_zero = np.isclose(umi, 0).sum(axis=1)
    max_zero = num_zero.argmax()
    plot_reconstruction(log_cpm.iloc[max_zero], reconstructed[:,max_zero])
  #+END_SRC

  #+RESULTS:
  :RESULTS:
  # Out[19]:
  [[file:figure/dim-reduction.org/reconstruction-error-max-zero.png]]
  :END:

  This is true even for genes with the lowest proportion of zero counts.

  #+BEGIN_SRC ipython :ipyfile figure/dim-reduction.org/reconstruction-error-min-zero.png
    min_zero = num_zero.argmin()
    plot_reconstruction(log_cpm.iloc[min_zero], reconstructed[:,min_zero])
  #+END_SRC

  #+RESULTS:
  :RESULTS:
  # Out[20]:
  [[file:figure/dim-reduction.org/reconstruction-error-min-zero.png]]
  :END:

  This might still be OK, if the reconstructed gene expression values are
  predictive of the original gene expression values.

  #+BEGIN_SRC ipython :async t
    pred_score = [sklm.LinearRegression(fit_intercept=True).fit(x.values.reshape(-1, 1), y).score(x.values.reshape(-1, 1), y)
                  for (_, x), (_, y)
                  in zip(log_cpm.iteritems(),
                         pd.DataFrame(reconstructed.T).iteritems())]
  #+END_SRC

  #+RESULTS:
  :RESULTS:
  # Out[21]:
  :END:

  #+BEGIN_SRC ipython :ipyfile figure/dim-reduction.org/reconstruction-pred-score.png
    plt.clf()
    plt.hist(pred_score, bins=50)
    plt.xlabel('Prediction $R^2$')
    plt.ylabel('Number of genes')
    plt.title('Correlation between PCA and original data')
  #+END_SRC

  #+RESULTS:
  :RESULTS:
  # Out[22]:
  : Text(0.5,1,'Correlation between PCA and original data')
  [[file:figure/dim-reduction.org/reconstruction-pred-score.png]]
  :END:

  The distribution of squared correlations suggest that the low rank
  approximation is better for some genes than others, i.e. that there could be
  gene-specific or gene module-specific effects. These are unlikely to be
  captured by PCA or factor analysis.

* Zero-inflated factor analysis

  If dropout changes the distribution of the non-zero observations, we might
  hope that fitting a latent variable model which explicitly includes dropout
  might eliminate that effect. Intuitively, we should downweight the evidence
  of observed zeroes on the inferred distribution which generated the
  observations. 

  However, fitting ZIFA ([[https://dx.doi.org/10.1186/s13059-015-0805-z][Pierson et al 2015]]) on the data again recovers a
  factor which is detection rate.

  #+BEGIN_SRC ipython :eval never :tangle zifa.py :noweb tangle
    <<dim-reduction-imports>>
    import ZIFA.block_ZIFA as zifa
    <<read-data-qc>>
    <<normalize>>
    latent, params = zifa.fitModel(Y=np.log(umi.values.T + 1), K=10, p0_thresh=.7)
    np.save('/project2/mstephens/aksarkar/projects/singlecell-qtl/data/zifa-loadings.npy', latent)
  #+END_SRC

  Run on 28 cores to complete in a reasonable amount of time.

  #+BEGIN_SRC sh :eval never-export :exports both
    sbatch --partition=broadwl --time=400 --mem=32G -n1 -c28 --exclusive --out=zifa.out --err zifa.err
    #!/bin/bash
    source activate scqtl
    python zifa.py
  #+END_SRC

  #+RESULTS:
  : Submitted batch job 44152013

  #+BEGIN_SRC sh :eval never-export :exports both :results output example
    sacct -j 44152013 -o Elapsed,MaxRSS,MaxVMSize
  #+END_SRC

  #+RESULTS:
  :    Elapsed     MaxRSS  MaxVMSize 
  : ---------- ---------- ---------- 
  :   02:54:50                       
  :   02:54:50  23391824K  27240528K 

  #+BEGIN_SRC ipython
    latent = np.load('/project2/mstephens/aksarkar/projects/singlecell-qtl/data/zifa-loadings.npy')
  #+END_SRC

  #+RESULTS:
  :RESULTS:
  # Out[75]:
  :END:

  #+BEGIN_SRC ipython :ipyfile figure/dim-reduction.org/zifa-covars.png
    corr_zifa = pd.concat(
      [correlation(pd.DataFrame(latent), cont_covars)] +
      [categorical_r2(latent, annotations, k) for k in cat_covars])
    corr_zifa = corr_zifa.pivot(index='covar', columns='pc')

    plt.clf()
    plt.gcf().set_size_inches(8, 12)
    im = plt.imshow(corr_zifa.values, cmap=colorcet.cm['fire'], vmin=0, vmax=1, aspect='auto')
    cb = plt.colorbar(im, orientation='horizontal')
    cb.set_label('Squared correlation')
    plt.xlabel('Learned factor')
    _ = plt.xticks(np.arange(latent.shape[1]), np.arange(1, latent.shape[1] + 1))
    plt.ylabel('Covariate')
    _ = plt.yticks(np.arange(corr_zifa.shape[0]), corr_zifa.index)
  #+END_SRC

  #+RESULTS:
  :RESULTS:
  # Out[58]:
  [[file:figure/dim-reduction.org/zifa-covars.png]]
  :END:

  ZIFA does not constrain factors to be orthogonal, so we would not expect it
  to get the same result as PPCA. However, the latent factor inferred by ZIFA
  correlated with sequencing depth still is highly correlated with PC1 inferred
  by PPCA, suggesting that it is not immune to whatever is biasing PPCA.

  #+BEGIN_SRC ipython :ipyfile figure/dim-reduction.org/pca-vs-zifa-corr.png
    plt.clf()
    plt.gcf().set_size_inches(12, 12)
    im = plt.imshow(np.tril(np.corrcoef(latent.T, loadings.T)), cmap=colorcet.cm['coolwarm'], vmin=-1, vmax=1)
    cb = plt.colorbar(im)
    cb.set_label('Correlation')
    labels = ['{} {}'.format(v, i) for v in ('Factor', 'PC') for i in range(10)]
    _ = plt.xticks(np.arange(20), labels, rotation=90)
    _ = plt.yticks(np.arange(20), labels)
  #+END_SRC

  #+RESULTS:
  :RESULTS:
  # Out[97]:
  [[file:figure/dim-reduction.org/pca-vs-zifa-corr.png]]
  :END:

* Effect of normalization on PCA

  Should we expect that the top principal component of log CPM is still
  sequencing depth?  [[https://dx.doi.org/10.1093/biostatistics/kxx053][Hicks et al 2017]] claim that correlation of the first
  principal component with detection rate can be explained by two facts:

  1. Centering the values of \(X\) does not center the values of \(\log X\)
     (and vice versa)
  2. \(E[\log X]\) depends on the gene detection rate

  To see whether these facts can explain the correlation between PC1 and
  sequencing metrics we see in our data, we perform the following simulation:

  1. Generate un-normalized relative expression values for two groups of
     samples, where one group has a true fold change in mean expression
     \(\beta\):

     \[ a^{(1)}_j \sim \mathrm{LogNormal}(0, 1) \]

     \[ a^{(2)}_0 = \beta a^{(1)}_0 \]

     \[ a^{(2)}_j = a^{(1)}_j,\ j \neq 0 \]

  2. Compute the relative expression for each group \(p^{(k)}_j = a^{(k)}_j /
     \sum_j a^{(k)}_j\)
  3. Sample a number of molecules \(R_i\) from the observed distribution of
     molecules
  4. Sample molecule counts \(x_{ij} \sim \mathrm{Multinomial}(R_i, p)\)
  5. Normalize to log CPM using the ~edgeR~ definition
  6. Fit probabilistic PCA ([[http://www.miketipping.com/papers/met-mppca.pdf][Tipping et al 1999]]) to explicitly account for mean
     differences in the normalized data:

     \[ p(x_i \mid z_i) \sim N(W z_i + \mu, \sigma^2 I) \]

     \[ p(z_i) \sim N(0, I) \]

     where:

     - \(x_i\) is a \(p \times 1\) observation
     - \(z_i\) is the associated \(k \times 1\) latent variable, \(k \ll p\)
     - \(W\) is the \(p \times k\) matrix of loadings
     - \(\mu\) is the \(p \times 1\) vector of gene-means

  7. Compute the squared correlation of loadings on each PC to \(R\)

  #+BEGIN_SRC ipython
    class Simulation:
      def __init__(self, num_genes, fold_change=2, seed=0):
        np.random.seed(seed)
        self.num_genes = num_genes
        rel_expr_1 = np.flip(np.sort(np.random.lognormal(size=self.num_genes)), axis=-1)
        rel_expr_2 = rel_expr_1.copy()
        rel_expr_2[0] *= fold_change
        self.rel_expr = [rel_expr_1, rel_expr_2]
        for r in self.rel_expr:
          r /= r.sum()

      def generate_log_cpm(self, num_samples, library_sizes, detection_rates=None):
        libsize = [np.random.choice(library_sizes, num_samples // 2) for _ in range(2)]
        if detection_rates is not None:
          det_rate = [np.random.choice(detection_rates, num_samples)]
        counts = np.array([np.random.multinomial(n, p) for sizes, p in zip(libsize, self.rel_expr) for n in sizes]).reshape(num_samples, self.num_genes)
        if detection_rates is not None:
          mask = np.array([np.random.uniform() < d for d in det_rate for _ in self.rel_expr[0]]).reshape(num_samples, self.num_genes)
          counts *= mask
        total_counts = counts.sum(axis=1)
        pseudocount = .5 * total_counts / total_counts.mean()
        log_cpm = np.log(counts + pseudocount.reshape(-1, 1)) - np.log(total_counts + 2 * pseudocount).reshape(-1, 1) + 6 * np.log(10)
        return log_cpm

      def pca_corr(num_components=10, num_trials=10):
        corr = []
        for _ in range(num_trials):
          ppca = skd.PCA(n_components=num_components)
          loadings = ppca.fit_transform(log_cpm)
          corr.append([st.pearsonr(x.ravel(), total_counts.ravel())[0] for x in loadings.T])
        return corr
  #+END_SRC

  #+RESULTS:
  :RESULTS:
  # Out[124]:
  :END:

  Look at a draw of the relative expression values:

  #+BEGIN_SRC ipython
    sim = Simulation(num_genes=1000)
  #+END_SRC

  #+RESULTS:
  :RESULTS:
  # Out[125]:
  :END:

  #+BEGIN_SRC ipython :ipyfile figure/dim-reduction.org/simulated-rel-expr.png
    plt.clf()
    plt.hist(sim.rel_expr[0], density=True, bins=50)
    plt.xlabel('Relative expression')
    plt.ylabel('Density')
  #+END_SRC

  #+RESULTS:
  :RESULTS:
  # Out[105]:
  : Text(0,0.5,'Density')
  [[file:figure/dim-reduction.org/simulated-rel-expr.png]]
  :END:

  Look at a draw of log CPM:

  #+BEGIN_SRC ipython :ipyfile figure/dim-reduction.org/simulated-log-cpm.png
    quantiles = np.linspace(0, 1, 9)[:-1]
    simulated_log_cpm = sim.generate_log_cpm(num_samples=50, library_sizes=annotations['molecules'])
    simulated_log_cpm = simulated_log_cpm[:,(quantiles * sim.num_genes).astype(int)]
    jitter = np.random.normal(scale=.01, size=(50, 1)) + quantiles.reshape(1, -1)
    plt.clf()
    plt.scatter(jitter.ravel(), simulated_log_cpm.ravel())
    plt.xlabel('Rank of relative gene expression')
    plt.ylabel('log CPM')
  #+END_SRC

  #+RESULTS:
  :RESULTS:
  # Out[139]:
  : Text(0,0.5,'log CPM')
  [[file:figure/dim-reduction.org/simulated-log-cpm.png]]
  :END:

  Although the first principal component has non-zero correlation with library
  size, our simulation does not produce correlations comparable to the
  correlation we see in the actual data. This result suggests that explanation
  (1), not centering the data, does not fully explain the correlation.

  #+BEGIN_SRC ipython :ipyfile figure/dim-reduction.org/simulated-pca.png :async t
    corr = simulate_pca_log_cpm(500, 1000, annotations['molecules'], fold_change=2, num_trials=20, seed=1)
    plt.clf()
    plt.boxplot(np.square(corr))
    plt.xlabel('Principal component')
    plt.ylabel('Squared correlation with library size')
  #+END_SRC

  #+RESULTS:
  :RESULTS:
  # Out[80]:
  : Text(0,0.5,'Squared correlation with library size')
  [[file:figure/dim-reduction.org/simulated-pca.png]]
  :END:

  In the simulation, the correlation does not appear to depend on the true fold
  change (i.e., proportion of variance explained by biological factors). This
  result could be explained if the proportion of variance explained by the true
  fold change were small compared to the proportion of variance explained by
  library size.

  #+BEGIN_SRC ipython :async t
    corr_vs_fold_change = [
      simulate_pca_log_cpm(500, 1000, annotations['molecules'], fold_change=fold_change, num_components=1, num_trials=20, seed=0)
      for fold_change in np.linspace(1.1, 2, 10)]
  #+END_SRC

  #+RESULTS:
  :RESULTS:
  # Out[128]:
  :END:

  #+BEGIN_SRC ipython :ipyfile figure/dim-reduction.org/simulated-pca-vs-fold-change.png
    plt.clf()
    plt.boxplot(np.square(np.array(corr_vs_fold_change).mean(axis=-1)).T, positions=np.arange(10))
    plt.xticks(np.arange(10), ['{:.3f}'.format(x) for x in np.linspace(1.1, 2, 10)])
    plt.xlabel('True fold change')
    plt.ylabel('Squared correlation with PC1')
  #+END_SRC

  #+RESULTS:
  :RESULTS:
  # Out[129]:
  : Text(0,0.5,'Squared correlation with PC1')
  [[file:figure/dim-reduction.org/simulated-pca-vs-fold-change.png]]
  :END:

  If (not) centering the data does not explain the correlation, does dropout
  explain the correlation? We simulated dropout after drawing molecule counts
  \(x_{ij}\) as follows:

  1. Draw gene detection rates \(q_i\) from the observed gene detection rates
  2. Draw \(h_{ij} \sim Bernoulli(q_i)\) iid.
  3. Use \(X^* = X \circ H\) as the observed count matrix

  Surprisingly, the correlation with the first PC goes away, likely because
  this dropout model is not correct.

  #+BEGIN_SRC ipython :async t
    corr_with_dropout = simulate_pca_log_cpm(500, 10000, annotations['molecules'], detection_rates=annotations['detect_hs'] / 2e4, fold_change=2, num_trials=20, seed=1)
  #+END_SRC

  #+RESULTS:
  :RESULTS:
  :END:

  #+BEGIN_SRC ipython :ipyfile figure/dim-reduction.org/simulated-pca-vs-dropout.png
    plt.clf()
    _ = plt.boxplot(np.square(corr_with_dropout))
    plt.xlabel('Principal component')
    plt.ylabel('Squared correlation with library size')
  #+END_SRC

  #+RESULTS:
  :RESULTS:
  # Out[84]:
  : Text(0,0.5,'Squared correlation with library size')
  [[file:figure/dim-reduction.org/simulated-pca-vs-dropout.png]]
  :END:

* Effect of dropout on gene expression

  Hicks et al also claim that the entire distribution of non-zero measurements
  depends on detection rate. They show this by plotting the percentiles of
  non-zero expressed genes in each cell versus detection rate in that cell.

  #+BEGIN_SRC ipython
    def plot_quantiles_vs_detection(umi, annotations, quantiles=None, pseudocount=None):
      if quantiles is None:
        quantiles = np.linspace(0, 1, 5)
      else:
        assert (quantiles >= 0).all()
        assert (quantiles <= 1).all()
      vals = np.nanpercentile(np.ma.masked_equal(umi.values, 0).astype(float).filled(np.nan), 100 * quantiles, axis=0, interpolation='higher')
      if pseudocount is None:
        # log CPM with per-cell pseudocount
        total_counts = umi.sum()
        pseudocount = .5 * total_counts / total_counts.mean()
        label = 'log CPM'
        vals = np.log(vals + pseudocount.values.reshape(1, -1)) - np.log(total_counts + 2 * pseudocount).values.reshape(1, -1) + 6 * np.log(10)
      else:
        vals = np.log(vals + pseudocount)
        label = '$\log(\mathrm{UMI} + {:.3g})$'.format(pseudocount)

      plt.clf()
      for q, v in zip(quantiles, vals):
        plt.scatter(annotations['detect_hs'] / umi.shape[0], v, c=colorcet.cm['inferno'](q), label='{:.3f}'.format(q))
      plt.legend(bbox_to_anchor=(1.04, .5), loc='center left')
      plt.xlabel('Detection rate')
      plt.ylabel(label)
  #+END_SRC

  #+RESULTS:
  :RESULTS:
  # Out[55]:
  :END:

  We recapitulate the main result of Hicks et al in our data.

  #+BEGIN_SRC ipython :ipyfile figure/dim-reduction.org/umi-quantiles-vs-detection.png
    plot_quantiles_vs_detection(umi, annotations)
  #+END_SRC

  #+RESULTS:
  :RESULTS:
  # Out[56]:
  [[file:figure/dim-reduction.org/umi-quantiles-vs-detection.png]]
  :END:

  log CPM as defined in ~edgeR~ uses a pseudocount which depends on library
  size, but the derivation in Hicks et al is for \(\log(X + \epsilon)\) where
  \(\epsilon\) is constant across cells. 

  Using constant \(\epsilon\) changes the shape of the relationship between
  quantiles of non-zero expression and detection rate, but does not remove the
  relationship.

  #+BEGIN_SRC ipython :ipyfile figure/dim-reduction.org/umi-quantiles-vs-detection-1.png
    plot_quantiles_vs_detection(umi, annotations, pseudocount=1)
  #+END_SRC

  #+RESULTS:
  :RESULTS:
  # Out[57]:
  [[file:figure/dim-reduction.org/umi-quantiles-vs-detection-1.png]]
  :END:

  #+BEGIN_SRC ipython :ipyfile figure/dim-reduction.org/umi-quantiles-vs-detection-1e-3.png
    plot_quantiles_vs_detection(umi, annotations, pseudocount=1e-3)
  #+END_SRC

  #+RESULTS:
  :RESULTS:
  # Out[58]:
  [[file:figure/dim-reduction.org/umi-quantiles-vs-detection-1e-3.png]]
  :END:

* PCA on bicentered data

  Bi-center the data, by fitting a model where observations depend on a
  row-mean and a column-mean.

  \[ x_{ij} \sim N(u_i + v_j, \sigma^2) \]

  #+BEGIN_SRC ipython
    def sample_feature_means(obs, max_iters=10):
      """Fit per-feature and per-sample means

      x_ij ~ N(u_i + v_j, sigma^2)

      Inputs:

      x - ndarray (num_samples, num_features)

      Returns:
      sample_means - ndarray (num_samples, 1)
      feature_means - ndarray (num_features, 1)

      """
      n, p = obs.shape
      sample_means = np.zeros((n, 1))
      feature_means = np.zeros((1, p))
      resid = obs.var()
      llik = -.5 * np.sum(np.log(2 * np.pi * resid) + (obs - feature_means - sample_means) / resid)
      # Coordinate ascent on llik
      for _ in range(max_iters):
        sample_means = np.mean(obs - feature_means, axis=1, keepdims=True)
        feature_means = np.mean(obs - sample_means, axis=0, keepdims=True)
        # By default, np divides by N, not N - 1
        resid = np.var(obs - feature_means - sample_means)
        update = -.5 * np.sum(np.log(2 * np.pi * resid) + (obs - feature_means - sample_means) / resid)
        print(update)
        if np.isclose(update, llik):
          return sample_means, feature_means.T
        else:
          llik = update
      raise ValueError("Failed to converge")
  #+END_SRC

  #+RESULTS:
  :RESULTS:
  # Out[8]:
  :END:

  #+CALL: normalize()

  #+RESULTS:
  :RESULTS:
  # Out[9]:
  :END:

  #+CALL: corr-def()

  #+RESULTS:
  :RESULTS:
  # Out[10]:
  :END:

  #+CALL: extract-covars()

  #+RESULTS:
  :RESULTS:
  # Out[11]:
  :END:

  #+CALL: plot-covars-def()

  #+RESULTS:
  :RESULTS:
  # Out[12]:
  :END:

  #+BEGIN_SRC ipython
    def plot_bicentered_pca(log_cpm, annotations, cont_covars, cat_covars):
      sample_means, feature_means = sample_feature_means(log_cpm.values.T)
      ppca = skd.PCA(n_components=10)
      loadings = ppca.fit_transform(log_cpm.values.T - sample_means - feature_means.T)
      corr = pd.concat(
        [correlation(pd.DataFrame(loadings), cont_covars)] +
        [categorical_r2(loadings, annotations, k) for k in cat_covars])
      corr = corr.pivot(index='covar', columns='pc')
      plot_pca_covar_corr(ppca, corr)
  #+END_SRC

  #+RESULTS:
  :RESULTS:
  # Out[13]:
  :END:

  #+BEGIN_SRC ipython :ipyfile figure/dim-reduction.org/pca-bicentered-covars.png
    plot_bicentered_pca(log_cpm, annotations, cont_covars, cat_covars)
  #+END_SRC

  #+RESULTS:
  :RESULTS:
  # Out[43]:
  [[file:figure/dim-reduction.org/pca-bicentered-covars.png]]
  :END:

  After bicentering the data, the subsequent top PC is still correlated with
  detection rate.

  #+BEGIN_SRC ipython
    corr_bicentered.loc['detect_hs', ('corr', 0)]
  #+END_SRC

  #+RESULTS:
  :RESULTS:
  # Out[49]:
  : 0.26792105510441244
  :END:

  The top PC is more strongly correlated with the percentiles of non-zero gene
  expression, suggesting that it is capturing the fact that the higher moments
  (shape) of the distribution of non-zero gene expression depends on gene
  detection rate.

  #+BEGIN_SRC ipython
    percentiles = np.nanpercentile(np.ma.masked_equal(umi.values, 0).astype(float).filled(np.nan), 100 * np.linspace(0, 1, 5), axis=0, interpolation='higher')
    normalized_percentiles = (np.log(percentiles + pseudocount.values.reshape(1, -1)) - np.log(libsize + 2 * pseudocount).values.reshape(1, -1) + 6 * np.log(10)) / np.log(2)
  #+END_SRC

  #+RESULTS:
  :RESULTS:
  # Out[38]:
  :END:

  #+BEGIN_SRC ipython :ipyfile figure/dim-reduction.org/pca-expr-percentile-corr.png
    plt.clf()
    im = plt.imshow(np.square(np.corrcoef(loadings_bicentered.T, normalized_percentiles)[10:,:10]), cmap=colorcet.cm['fire'], vmin=0, vmax=1)
    cb = plt.colorbar(im, orientation='horizontal')
    cb.set_label('Squared correlation')
    plt.xlabel('Principal component')
    plt.ylabel('Percentile of non-zero log CPM')
    _ = plt.xticks(np.arange(10), np.arange(1, 11))
    _ = plt.yticks(np.arange(5), ['{:.2f}'.format(x) for x in np.linspace(0, 1, 5)])
    plt.gcf().tight_layout()
  #+END_SRC

  #+RESULTS:
  :RESULTS:
  # Out[84]:
  [[file:figure/dim-reduction.org/pca-expr-percentile-corr.png]]
  :END:

* Kernel PCA

  The dependency of non-zero gene expression on gene detection rate is
  non-linear, so use kernel PCA to perform non-linear dimensionality reduction
  ([[https://www.mitpressjournals.org/doi/10.1162/089976698300017467][SchÃ¶lkopf et al 1998]]). The basic idea is to non-linearly map the original
  points into a different space, and perform PCA in that space.

  The method eliminates the second technical PC by accurately modeling the
  non-linearity in the data, but it fails to eliminate the first technical PC
  because it does not include sample-specific mean parameters. 

  It is non-trivial to add such parameters because we have to center the
  projections of the samples, and the key algorithmic trick used is that we
  never have to actually compute the projections. In particular, we assume the
  radial basis function kernel, which projects the data into infinite
  dimensional space, making it impossible to compute the projections.

  #+BEGIN_SRC ipython :async t
    kpca = skd.KernelPCA(kernel='rbf', n_components=10)
    loadings_kpca = kpca.fit_transform(log_cpm.values.T)
    corr_kpca = pd.concat(
      [correlation(pd.DataFrame(loadings_kpca), cont_covars)] +
      [categorical_r2(loadings_kpca, annotations, k) for k in cat_covars])
    corr_kpca = corr_kpca.pivot(index='covar', columns='pc')
  #+END_SRC

  #+RESULTS:
  :RESULTS:
  # Out[15]:
  :END:

  #+BEGIN_SRC ipython :ipyfile figure/dim-reduction.org/kpca-covars.png
    plt.clf()
    plt.gcf().set_size_inches(8, 12)
    im = plt.imshow(corr_kpca.values, cmap=colorcet.cm['fire'], vmin=0, vmax=1, aspect='auto')
    cb = plt.colorbar(im, orientation='horizontal')
    cb.set_label('Squared correlation')
    plt.gca().set_xlabel('Principal component')
    plt.gca().set_yticks(np.arange(corr_kpca.shape[0]))
    plt.gca().set_yticklabels(corr_kpca.index)
    plt.gca().set_ylabel('Covariate')
  #+END_SRC

  #+RESULTS:
  :RESULTS:
  # Out[20]:
  : Text(0,0.5,'Covariate')
  [[file:figure/dim-reduction.org/kpca-covars.png]]
  :END:

* PCA on gene expression residuals

  Although the dependency of the percentiles of non-zero gene expression
  appears to be nonlinear, we can partially correct for it by regressing out
  the percentiles of expression from the expression values for each gene.

  \[ y_{ij} = p_i \beta + \mu_j + \epsilon_{ij} \]

  \[ \tilde{y}_{ij} = y_{ij} - p_i \hat\beta - \hat\mu_j \]

  #+BEGIN_SRC ipython :async t
    log_cpm_residuals = log_cpm.apply(lambda y: y - sklm.LinearRegression(fit_intercept=True).fit(normalized_percentiles.T, y).predict(normalized_percentiles.T), axis=1)
  #+END_SRC

  #+RESULTS:
  :RESULTS:
  # Out[43]:
  :END:

  #+BEGIN_SRC ipython :ipyfile figure/dim-reduction.org/pca-expr-residual-covars.png
    plot_bicentered_pca(log_cpm_residuals, annotations, cont_covars, cat_covars)
  #+END_SRC

  #+RESULTS:
  :RESULTS:
  # Out[48]:
  [[file:figure/dim-reduction.org/pca-expr-residual-covars.png]]
  :END:
